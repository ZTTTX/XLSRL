{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, List\n",
    "\n",
    "from collections import namedtuple\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import isomorphism\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from math import ceil, log2\n",
    "\n",
    "from __future__ import annotations\n",
    "from egglog import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, name=None, bitwidth=None, operation=None, operands=None, \n",
    "                 idNum=None, value=-1, pos=None, FuncIO=None, start=None, width=None, array_sizes=None, indices=None):\n",
    "        self.name = name\n",
    "        self.bitwidth = bitwidth\n",
    "        self.operation = operation\n",
    "        self.operands = operands\n",
    "        self.idNum = idNum\n",
    "        self.value = value\n",
    "        self.pos = pos\n",
    "        self.FuncIO = FuncIO\n",
    "        self.start = start\n",
    "        self.width = width\n",
    "        self.array_sizes = array_sizes\n",
    "        self.indices = indices\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Node(name={self.name}, bitwidth={self.bitwidth}, operation={self.operation}, operands={self.operands}, id={self.idNum}, value={self.value}, pos={self.pos}, FuncIO={self.FuncIO}, start={self.start}, width={self.width}, array_sizes={self.array_sizes}, indices={self.indices})\"\n",
    "\n",
    "def node_to_dict(node):\n",
    "    return {\n",
    "        \"OperationName\": node.name,\n",
    "        \"BitWidth\": node.bitwidth,\n",
    "        \"OperationType\": node.operation,\n",
    "        \"Operands\": node.operands,\n",
    "        \"idNum\": node.idNum,\n",
    "        \"Value\": node.value,\n",
    "        \"Pos\": node.pos,\n",
    "        \"FuncIO\": node.FuncIO,\n",
    "        \"Start\": node.start,\n",
    "        \"Width\": node.width,\n",
    "        \"ArraySize\": node.array_sizes,\n",
    "        \"Indices\": node.indices\n",
    "    }\n",
    "    \n",
    "def ParseIRFile(ir_text):\n",
    "    nodes_dict = {}\n",
    "    lines = ir_text.strip().split(\"\\n\")[:]\n",
    "    return lines\n",
    "\n",
    "def LineToDict(line):\n",
    "#     print(line)\n",
    "    nodes = []\n",
    "    # Check for function header\n",
    "    FuncIO = \"Not\"\n",
    "    if \"fn\" in line:\n",
    "        FuncIO = \"In\"\n",
    "        signature_pattern = re.compile(r\"fn\\s+(\\w+)\\((.*?)\\)\\s*->\\s*(.*?)\\s*\\{\")\n",
    "        signature_match = signature_pattern.search(line)\n",
    "        if signature_match:\n",
    "            func_name = signature_match.group(1)\n",
    "            params = signature_match.group(2).split(',')\n",
    "            for param in params:\n",
    "                param_name, param_type = param.split(':')\n",
    "                param_name = param_name.strip()\n",
    "                array_sizes = [int(size) for size in re.findall(r'\\[(\\d+)\\]', param_type)]\n",
    "                if array_sizes == []:\n",
    "                    array_sizes = None\n",
    "                bitwidth = int(array_sizes.pop(0))  # The first size is the bitwidth\n",
    "                node = Node(name=param_name, bitwidth=bitwidth, array_sizes=array_sizes, FuncIO=\"In\")\n",
    "                nodes.append(node)\n",
    "        return nodes\n",
    "\n",
    "    if \"ret\" in line:\n",
    "        FuncIO = \"Out\"\n",
    "        line = line[4:]\n",
    "        \n",
    "    if line == \"}\":\n",
    "        return None\n",
    "\n",
    "    # Extract node name\n",
    "#     name = re.search(r\"(\\w+\\.\\d+):\", line).group(1)\n",
    "    name = re.search(r\"(\\w+(\\.\\d+)?)\\:\", line).group(1)\n",
    "\n",
    "    # Extract bitwidth and array size\n",
    "    array_pattern = re.compile(r\"bits\\[(\\d+)\\]((?:\\[\\d+\\])*)\")\n",
    "    array_match = array_pattern.search(line)\n",
    "    if array_match:\n",
    "        bitwidth = int(array_match.group(1))\n",
    "        array_sizes_str = array_match.group(2)\n",
    "        array_sizes = [int(size) for size in re.findall(r'\\[(\\d+)\\]', array_sizes_str)]\n",
    "        if array_sizes == []:\n",
    "            array_sizes = None\n",
    "    \n",
    "    # Extract operation\n",
    "    operation = re.search(r\"= (\\w+)\", line).group(1)\n",
    "\n",
    "    # Extract operands; \n",
    "    # Eidt to ignore other argument keywords\n",
    "    operands = re.findall(r\"(\\w+\\.\\d+|\\w+)\", line.split(\"=\")[1])\n",
    "    operands = [op for op in operands if op not in [operation, \"value\", \"id\", \"pos\", \"start\", \"width\", \"indices\"]]\n",
    "\n",
    "    # Extract indices: only applies for array_indice node\n",
    "    indices = None\n",
    "    indices_match = re.search(r\"indices=\\[(.*?)\\]\", line)\n",
    "    if indices_match:\n",
    "        # Get the string of indices\n",
    "        indices_str = indices_match.group(1)\n",
    "        # Find all occurrences of literals or node names in the indices string\n",
    "        indices = re.findall(r\"(\\w+\\.\\d+|\\w+)\", indices_str)\n",
    "    \n",
    "    # Extract id\n",
    "    idNum = int(re.search(r\"id=(\\d+)\", line).group(1))\n",
    "\n",
    "    # Extract value (if present)\n",
    "    value_match = re.search(r\"value=(\\d+)\", line)\n",
    "    value = int(value_match.group(1)) if value_match else -1\n",
    "\n",
    "    # Extract pos (if present)\n",
    "    pos_match = re.search(r\"pos=\\[\\((\\d+,\\d+,\\d+)\\)\\]\", line)\n",
    "    pos = tuple(map(int, pos_match.group(1).split(\",\"))) if pos_match else None\n",
    "\n",
    "    # Extract start and width for bit_slice (if present)\n",
    "    start = None\n",
    "    width = None\n",
    "    if operation == \"bit_slice\":\n",
    "        start = int(re.search(r\"start=(\\d+)\", line).group(1))\n",
    "        width = int(re.search(r\"width=(\\d+)\", line).group(1))\n",
    "\n",
    "    # Create Node object\n",
    "    node = Node(name, bitwidth, operation, operands, idNum, value, pos, FuncIO, start, width, array_sizes, indices)\n",
    "    \n",
    "    # Add to node list\n",
    "    nodes.append(node)\n",
    "    return nodes\n",
    "\n",
    "def DictToGraph(G, NodeDict):\n",
    "    # This function turns the node dictionary to directional graph\n",
    "    NodeList = []\n",
    "    EdgeList = []\n",
    "    for NodeName in NodeDict:\n",
    "        if NodeDict[NodeName].FuncIO != \"In\": \n",
    "            # This if is for handling the function top input, they are treated as nodes as well.\n",
    "            NodeList.append((NodeDict[NodeName].idNum, node_to_dict(NodeDict[NodeName])))\n",
    "            for ParentName in NodeDict[NodeName].operands:\n",
    "                if NodeDict[ParentName].FuncIO != \"In\":\n",
    "                    EdgeList.append((NodeDict[ParentName].idNum, NodeDict[NodeName].idNum))\n",
    "                else: \n",
    "                    EdgeList.append((NodeDict[ParentName].name, NodeDict[NodeName].idNum))\n",
    "            if NodeDict[NodeName].indices != None:\n",
    "                for ParentName in NodeDict[NodeName].indices:\n",
    "                    if NodeDict[ParentName].FuncIO != \"In\":\n",
    "                        EdgeList.append((NodeDict[ParentName].idNum, NodeDict[NodeName].idNum))\n",
    "                    else: \n",
    "                        EdgeList.append((NodeDict[ParentName].name, NodeDict[NodeName].idNum))\n",
    "        else:\n",
    "            NodeList.append((NodeDict[NodeName].name, node_to_dict(NodeDict[NodeName])))\n",
    "    G.add_nodes_from(NodeList)\n",
    "    G.add_edges_from(EdgeList)\n",
    "    return G\n",
    "    \n",
    "    \n",
    "def slice_ir_by_function(ir_content):\n",
    "    # Regular expression to match function definitions with and without \"top\"\n",
    "    fn_pattern = re.compile(r\"(?:top\\s+)?fn\\s+(\\w+)\\((.*?)\\)\\s*->\\s*(.*?)\\s*\\{\")\n",
    "\n",
    "    # Split the content by lines for processing\n",
    "    lines = ir_content.split('\\n')\n",
    "\n",
    "    # Dictionary to store each function's IR content\n",
    "    functions_dict = {}\n",
    "\n",
    "    # Buffer to store current function lines\n",
    "    current_fn = None\n",
    "    current_fn_lines = []\n",
    "\n",
    "    # Iterate through each line\n",
    "    for line in lines:\n",
    "        # Check if the line starts a new function definition\n",
    "        match = fn_pattern.match(line)\n",
    "        if match:\n",
    "            # If there is a current function being processed, save it\n",
    "            if current_fn:\n",
    "                functions_dict[current_fn] = '\\n'.join(current_fn_lines)\n",
    "                current_fn_lines = []\n",
    "            # Start a new function\n",
    "            current_fn = match.group(1)\n",
    "        # Add the line to the current function's lines\n",
    "        if current_fn:\n",
    "            current_fn_lines.append(line)\n",
    "        # Check if the line ends a function definition\n",
    "        if line.strip() == '}':\n",
    "            # Save the current function and reset\n",
    "            if current_fn:\n",
    "                functions_dict[current_fn] = '\\n'.join(current_fn_lines)\n",
    "                current_fn = None\n",
    "                current_fn_lines = []\n",
    "\n",
    "    # Return the dictionary with function names as keys and IR content as values\n",
    "    return functions_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_registers_in_block(ir_content):\n",
    "    # Flag to check if we are inside a 'block' section\n",
    "    in_block_section = False\n",
    "    register_count = 0\n",
    "\n",
    "    # Splitting the content into lines\n",
    "    lines = ir_content.splitlines()\n",
    "\n",
    "    for line in lines:\n",
    "        # Check if the 'block' section starts\n",
    "        if line.strip().startswith(\"block \"):\n",
    "            in_block_section = True\n",
    "        # Check if the 'block' section ends\n",
    "        elif line.strip() == \"}\" and in_block_section:\n",
    "            in_block_section = False\n",
    "        # Count the registers if inside a 'block' section\n",
    "        elif in_block_section and line.strip().startswith(\"reg \"):\n",
    "            register_count += 1\n",
    "\n",
    "    return register_count\n",
    "\n",
    "def ReadScheduleIR(file_path):\n",
    "    TopFunctionName = None\n",
    "    with open(file_path, 'r') as f:\n",
    "        ir_content = f.read()\n",
    "        ir_dict = slice_ir_by_function(ir_content)\n",
    "    FuncNodeDict = {}\n",
    "    for fn_name, fn_content in ir_dict.items():\n",
    "        NodeDict = {}\n",
    "        Lines = ParseIRFile(fn_content)\n",
    "        for Line in Lines:    \n",
    "            NodeList = LineToDict(Line)\n",
    "            if NodeList != None:\n",
    "                for NodeObj in NodeList:\n",
    "                    NodeDict[NodeObj.name] = NodeObj\n",
    "            if \"top\" in Line:\n",
    "                TopFunctionName = fn_name\n",
    "        FuncNodeDict[fn_name] = NodeDict\n",
    "    SubCounter = 0\n",
    "    NodeCounter = 0\n",
    "    JsonOutDict = {}\n",
    "    G_goble = nx.DiGraph()\n",
    "    for fn_name, fn_nodes in FuncNodeDict.items():\n",
    "        G = nx.DiGraph()\n",
    "        G_goble = DictToGraph(G_goble, fn_nodes)\n",
    "        \n",
    "    register_count = count_registers_in_block(ir_content)\n",
    "    return G_goble, register_count\n",
    "\n",
    "\n",
    "def read_SDC_pipeline_result(file_path):\n",
    "    max_stage_latency = 0\n",
    "    with open(file_path, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "\n",
    "    schedule_dict = {}\n",
    "    current_stage = None\n",
    "    current_node = None\n",
    "    \n",
    "    # Regular expressions to match the lines\n",
    "    function_pattern = re.compile(r'^function: \"(.*)\"')\n",
    "    stage_pattern = re.compile(r'^\\s*stage: (\\d+)')\n",
    "    node_pattern = re.compile(r'^\\s*node: \"(.*)\"')\n",
    "    node_delay_pattern = re.compile(r'^\\s*node_delay_ps: (\\d+)')\n",
    "    path_delay_pattern = re.compile(r'^\\s*path_delay_ps: (\\d+)')\n",
    "\n",
    "    for line in file_contents.splitlines():\n",
    "        # Check for function\n",
    "        function_match = function_pattern.match(line)\n",
    "        if function_match:\n",
    "            schedule_dict['function'] = function_match.group(1)\n",
    "            schedule_dict['stages'] = []\n",
    "            continue\n",
    "        \n",
    "        # Check for stage\n",
    "        stage_match = stage_pattern.match(line)\n",
    "        if stage_match:\n",
    "            current_stage = {'stage': int(stage_match.group(1)), 'timed_nodes': []}\n",
    "            schedule_dict['stages'].append(current_stage)\n",
    "            continue\n",
    "        \n",
    "        # Check for node\n",
    "        node_match = node_pattern.match(line)\n",
    "        if node_match:\n",
    "            current_node = {'node': node_match.group(1)}\n",
    "            current_stage['timed_nodes'].append(current_node)\n",
    "            continue\n",
    "        \n",
    "        # Check for node delay\n",
    "        node_delay_match = node_delay_pattern.match(line)\n",
    "        if node_delay_match:\n",
    "            current_node['node_delay_ps'] = int(node_delay_match.group(1))\n",
    "            continue\n",
    "        \n",
    "        # Check for path delay\n",
    "        path_delay_match = path_delay_pattern.match(line)\n",
    "        if path_delay_match:\n",
    "            cur_path_delay = int(path_delay_match.group(1))\n",
    "            current_node['path_delay_ps'] = cur_path_delay\n",
    "            if cur_path_delay > max_stage_latency:\n",
    "                max_stage_latency = cur_path_delay\n",
    "            continue\n",
    "        \n",
    "    return schedule_dict, max_stage_latency\n",
    "\n",
    "def register_SDC_result(G, schedule_dict):\n",
    "    stage_num = 0\n",
    "    for cur_dict in schedule_dict['stages']:\n",
    "        stage_num += 1\n",
    "        cur_stage = cur_dict['stage']\n",
    "        node_list = cur_dict['timed_nodes']\n",
    "        for cur_node in node_list:\n",
    "            # Check if the last part is an integer\n",
    "            node_name_parts = cur_node['node'].split('.')\n",
    "            if node_name_parts[-1].isdigit():\n",
    "                # If it is, use it as the ID\n",
    "                cur_node_id = int(node_name_parts[-1])\n",
    "            else:\n",
    "                # If not, use the entire node name as the ID\n",
    "                cur_node_id = cur_node['node']\n",
    "            \n",
    "            G.nodes[cur_node_id]['node_delay_ps'] = cur_node['node_delay_ps']\n",
    "            G.nodes[cur_node_id]['path_delay_ps'] = cur_node['path_delay_ps']\n",
    "            G.nodes[cur_node_id]['stage'] = cur_stage\n",
    "    return G, stage_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_unify_name(ir_input_path, ir_unify_out):\n",
    "    command = [\n",
    "        \"/home/xlsrl/XLSRL/bazel-bin/xls/tools/UnifyName\",\n",
    "        ir_input_path,\n",
    "        ir_unify_out\n",
    "    ]\n",
    "    command_str = \" \".join(command)\n",
    "    try:\n",
    "        result = subprocess.run(command_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        result.check_returncode()  # This will raise an exception if the return code was non-zero\n",
    "        print(\"Name Unification Run Done.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"An error occurred while running the command.\")\n",
    "        print(\"Error message:\", e.stderr)\n",
    "    return\n",
    "\n",
    "def run_rewriter(command_executable, ir_input_path, json_output_path, ir_output_path):\n",
    "    print(\"\\nRunning Standalone Rewriter\")\n",
    "    command = f\"{command_executable} {ir_input_path} {json_output_path} {ir_output_path}\"\n",
    "    try:\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        result.check_returncode()\n",
    "        print(\"Rewriter Run Done\")\n",
    "#         print(\"Output:\", result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"An error occurred while running the command.\")\n",
    "        print(\"Error message:\", e.stderr)\n",
    "    return\n",
    "\n",
    "def run_sdc_scheduler(ir_output_path, output_verilog_path, schedule_result_path, output_schedule_ir_path, \n",
    "                      delay_model, TopFunctionName, clock_period_ps, clock_margin_precent, \n",
    "                      pipeline_stages, period_relaxation_percent):\n",
    "    command = [\n",
    "            \"/home/xlsrl/XLSRL/bazel-bin/xls/tools/codegen_main\",\n",
    "            ir_output_path,\n",
    "            '--generator=pipeline',  \n",
    "            f'--delay_model={delay_model}',\n",
    "            '--module_name=xls_test',  # Assuming module_name is static\n",
    "            f'--top={TopFunctionName}',\n",
    "            f'--output_verilog_path={output_verilog_path}',  \n",
    "            f'--output_schedule_path={schedule_result_path}',\n",
    "            f'--output_schedule_ir_path={output_schedule_ir_path}'  \n",
    "        ]\n",
    "        \n",
    "    if clock_period_ps != None:\n",
    "        command.append(f'--clock_period_ps={clock_period_ps}')\n",
    "        if clock_margin_precent != None:\n",
    "            command.append(f'--clock_margin_percent={clock_margin_precent}')\n",
    "    elif pipeline_stages != None:\n",
    "        command.append(f'--pipeline_stages={pipeline_stages}')\n",
    "        if period_relaxation_percent != None:\n",
    "            command.append(f'--period_relaxation_percent={period_relaxation_percent}')\n",
    "\n",
    "    command_str = \" \".join(command)\n",
    "    try:\n",
    "        result = subprocess.run(command_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        result.check_returncode()  # This will raise an exception if the return code was non-zero\n",
    "        print(\"Scheduling Run Done.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"An error occurred while running the command.\")\n",
    "        print(\"Error message:\", e.stderr)\n",
    "    return\n",
    "\n",
    "def get_graph_with_scheduling(output_schedule_ir_path, schedule_result_path):\n",
    "    print(\"Reading Scheduling Result\")\n",
    "    G_schedule, register_count = ReadScheduleIR(output_schedule_ir_path)\n",
    "    schedule_dict, max_stage_latency = read_SDC_pipeline_result(schedule_result_path)\n",
    "#     print(schedule_dict)\n",
    "    G_schedule, stage_num = register_SDC_result(G_schedule, schedule_dict)\n",
    "    print(\"Result Graph Generation Done\")\n",
    "    return G_schedule, max_stage_latency, stage_num, register_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entries(operand_list, to_remove):\n",
    "    return [op for op in operand_list if op not in to_remove]\n",
    "\n",
    "def traverse_graph(G, start_node, processed_nodes):\n",
    "# This function takes in a graph and partitions it to the largest legal rewrite batch\n",
    "# If a node is not in the allowed list, it is discarded.\n",
    "    allowed_operations = [\"add\", \"umul\", \"smul\", \"shll\", \"shrl\", \"not\", \"neg\", \"sub\", \"udiv\", \"sdiv\"] # \"literal\",\n",
    "    # Initialize the batch list\n",
    "    batch = []\n",
    "\n",
    "    # Create a set to keep track of visited nodes\n",
    "    visited = set()\n",
    "\n",
    "    # Traverse the graph\n",
    "    stack = [start_node]\n",
    "    while stack:\n",
    "#         print(stack)\n",
    "        current = stack.pop()\n",
    "        current_op_type = G.nodes[current]['OperationType']\n",
    "\n",
    "        # Check if the current node's operation is allowed\n",
    "        if current_op_type in allowed_operations:\n",
    "            # Add the current node to the batch and mark as visited\n",
    "            batch.append(current)\n",
    "            visited.add(current)\n",
    "\n",
    "            for parent in G.predecessors(current):\n",
    "                # Check if all children of the parent are either visited or disallowed\n",
    "                all_children_allowed = all(\n",
    "                    child in visited # or G.nodes[child].get('OperationType', None) not in allowed_operations\n",
    "                    for child in G.successors(parent)\n",
    "                )\n",
    "\n",
    "                if all_children_allowed and parent not in processed_nodes and parent not in visited:\n",
    "                    stack.append(parent)\n",
    "    if len(batch) == 1 and G.nodes[batch[0]]['OperationType'] == 'literal':\n",
    "        batch = []\n",
    "    return batch\n",
    "\n",
    "def get_expr_from_batch(G, batch):\n",
    "    N = len(batch)\n",
    "    max_loop_count = N * (N + 1) // 2\n",
    "    batch_queue = deque(batch)\n",
    "    visited_dict = {}\n",
    "    loop_count = 0\n",
    "    cur_expr = None\n",
    "    is_unsigned = True\n",
    "    while batch_queue:\n",
    "        cur_expr = None\n",
    "        cur_node = batch_queue.pop()\n",
    "        # If all operands are not in the batch waiting the be handled, or they have been vistied before\n",
    "        if all(operand not in batch or operand in visited_dict for operand in G.nodes[cur_node]['Operands']):\n",
    "            if G.nodes[cur_node]['OperationType'] == 'literal':\n",
    "                visited_dict[G.nodes[cur_node]['OperationName']] = \"Num.(\" + str(G.nodes[cur_node]['Value']) + \")\"\n",
    "            else:\n",
    "                cur_expr = G.nodes[cur_node]['OperationType'] + \"(\"\n",
    "                for operand in G.nodes[cur_node]['Operands']:\n",
    "                    # If the operand belongs to a processed expr, use it. Otherwise use the operand as string\n",
    "                    if operand in visited_dict.keys():\n",
    "                        cur_expr = cur_expr +  visited_dict[operand] + \",\"\n",
    "                    else:\n",
    "                        cur_expr = cur_expr + \"Num.var('\" + operand + \"')\" + \",\"\n",
    "                cur_expr = cur_expr[:-1] + \")\"\n",
    "                visited_dict[G.nodes[cur_node]['OperationName']] = cur_expr\n",
    "            \n",
    "            if G.nodes[cur_node]['OperationType'] in [\"neg\", \"smul\", \"sdiv\"]:\n",
    "                is_unsigned = False\n",
    "            \n",
    "        else:\n",
    "            batch_queue.appendleft(cur_node)\n",
    "            \n",
    "        if loop_count > max_loop_count:\n",
    "            break\n",
    "        else:\n",
    "            loop_count += 1\n",
    "    \n",
    "    # Handle sign mapping, handle not mapping\n",
    "    cur_expr = cur_expr.replace(\"umul\", \"mul\").replace(\"smul\", \"mul\")\n",
    "    cur_expr = cur_expr.replace(\"udiv\", \"div\").replace(\"sdiv\", \"div\")\n",
    "    cur_expr = cur_expr.replace(\"not\", \"not_\")\n",
    "    cur_expr = cur_expr.replace(\"neg\", \"neg_\")\n",
    "    if len(batch_queue) == 0: \n",
    "        # Return the one with the most dependency\n",
    "        return cur_expr, is_unsigned\n",
    "    else:\n",
    "        print(\"[ERROR] Unresolved dependency in batch to expr generation\")\n",
    "        return None, None\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def gen_json_from_expr_recursive(G, expr, operation_mapping, bit_width, nodes_involved_dict=None, counter=None):\n",
    "    # print(\"Get Expr: \", expr)\n",
    "    if counter is None:\n",
    "        counter = 0\n",
    "    if nodes_involved_dict is None:\n",
    "        nodes_involved_dict = {}\n",
    "    \n",
    "    match = re.search(r'\\b(\\w+)\\(([^()]+)\\)', expr)\n",
    "    if not match:\n",
    "        return nodes_involved_dict, expr\n",
    "\n",
    "    operation_type, operands = match.groups()\n",
    "    operation_name = 'auto_gen'+str(counter)\n",
    "\n",
    "    # Replace the innermost expression with the placeholder\n",
    "    new_expr = expr.replace(match.group(0), operation_name)\n",
    "    # print(\"OpType: \", operation_type, \"; Operands: \",operands)\n",
    "    # print(\"Iter:\", counter, \"expr: \",new_expr)\n",
    "    \n",
    "    \n",
    "    # Handle each node:\n",
    "    nodes_involved_dict, counter = handle_node_generation(G, operation_type, operands, operation_name, bit_width, nodes_involved_dict, operation_mapping, counter)\n",
    "    return gen_json_from_expr_recursive(G, new_expr, operation_mapping, bit_width, nodes_involved_dict, counter+1)\n",
    "\n",
    "\n",
    "# Node(name=None, bitwidth=None, operation=None, operands=None, \n",
    "#                  idNum=None, value=None, pos=None, FuncIO=None, start=None, width=None)\n",
    "\n",
    "def handle_node_generation(G, operation_type, operands, operation_name, bit_width, nodes_involved_dict, operation_mapping, counter):\n",
    "    # operands_list = operands.split(',')\n",
    "    operands_list = [op.strip() for op in operands.replace(\"\\n\", \"\").split(',') if op.strip()]\n",
    "    new_node_operands = []\n",
    "    NewNode = None\n",
    "    for cur_operand in operands_list:\n",
    "        # First setup operand list\n",
    "        if cur_operand.strip(\"'\").strip(\" \").isdigit():\n",
    "            counter += 1\n",
    "            # Handle pure digit, generate literal from them\n",
    "            try:\n",
    "                cur_value = int(cur_operand.strip(\"'\").strip(\" \"))\n",
    "            except ValueError:\n",
    "                cur_value = float(cur_operand.strip(\"'\").strip(\" \"))\n",
    "                \n",
    "            # Update the bitwidth to the nearest power of 2 if necessary\n",
    "            required_bitwidth = ceil(log2(abs(cur_value) + 1)) if cur_value >= 0 else ceil(log2(abs(cur_value)))\n",
    "#             print(cur_value, ' ', bit_width, \" \" , required_bitwidth)\n",
    "\n",
    "            if required_bitwidth > bit_width:\n",
    "                bit_width = 2 ** ceil(log2(required_bitwidth))    \n",
    "            NewLiteralNode = node_to_dict(Node('auto_gen'+str(counter), bit_width, 'Literal', value=cur_value))\n",
    "            NewLiteralNode[\"ReplaceSelfWith\"] = 'Gen'       \n",
    "            nodes_involved_dict['auto_gen'+str(counter)] = NewLiteralNode\n",
    "            new_node_operands.append('auto_gen'+str(counter))\n",
    "        else:\n",
    "            new_node_operands.append(cur_operand.strip(\"'\"))\n",
    "    #Now generate new node:\n",
    "    # print(\"OperationType:\" , operation_type)\n",
    "    \n",
    "    NewNode = node_to_dict(Node(operation_name, bit_width, operation_mapping[operation_type], new_node_operands))\n",
    "    NewNode[\"ReplaceSelfWith\"] = 'Gen'\n",
    "    nodes_involved_dict[operation_name] = NewNode\n",
    "    return nodes_involved_dict, counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[add(Num.var(\"a\"), Num.var(\"a\")),\n",
       " mul(Num.var(\"a\"), Num(2)),\n",
       " mul(Num(2), Num.var(\"a\")),\n",
       " shll(Num.var(\"a\"), Num(1))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_egraph_in_range(expr, max_iters=10):\n",
    "    egraph = EGraph()\n",
    "    \n",
    "    @egraph.class_\n",
    "    class Num(Expr):\n",
    "        def __init__(self, value: i64Like) -> None:\n",
    "            ...\n",
    "    \n",
    "        @classmethod\n",
    "        def var(cls, name: StringLike) -> Num:\n",
    "            ...\n",
    "\n",
    "    # Define custom functions for operations\n",
    "    @egraph.function\n",
    "    def add(x: Num, y: Num) -> Num:\n",
    "        ...\n",
    "    \n",
    "    @egraph.function\n",
    "    def sub(x: Num, y: Num) -> Num:\n",
    "        ...\n",
    "    \n",
    "    @egraph.function\n",
    "    def mul(x: Num, y: Num) -> Num:\n",
    "        ...\n",
    "    \n",
    "    @egraph.function\n",
    "    def div(x: Num, y: Num) -> Num:\n",
    "        ...\n",
    "    \n",
    "    @egraph.function\n",
    "    def shll(x: Num, y: Num) -> Num:\n",
    "        ...\n",
    "    \n",
    "    @egraph.function\n",
    "    def shrl(x: Num, y: Num) -> Num:\n",
    "        ...\n",
    "    \n",
    "    @egraph.function\n",
    "    def not_(x: Num) -> Num:\n",
    "        ...\n",
    "    \n",
    "    @egraph.function\n",
    "    def neg_(x: Num) -> Num:\n",
    "        ...\n",
    "\n",
    "# Rewrite rules\n",
    "    @egraph.register\n",
    "    def custom_rules(a: Num, b: Num, c: Num, i: i64, j: i64):\n",
    "        # Basic arithmetic rules\n",
    "        yield rewrite(add(a, b)).to(add(b, a))\n",
    "        yield rewrite(mul(a, b)).to(mul(b, a))\n",
    "        yield rewrite(add(add(a, b), c)).to(add(b, add(a, c)))  # Additive associativity\n",
    "        yield rewrite(mul(a, add(b, c))).to(add(mul(a, b), mul(a, c)))  # Multiplicative distributivity\n",
    "        yield rewrite(add(a, Num(0))).to(a)\n",
    "        yield rewrite(mul(a, Num(0))).to(Num(0))\n",
    "        yield rewrite(mul(a, Num(1))).to(a)\n",
    "        yield rewrite(sub(a, Num(0))).to(a)\n",
    "        yield rewrite(neg_(a)).to(sub(Num(0), a))\n",
    "    \n",
    "        # Bitwise rules\n",
    "        yield rewrite(shll(a, Num(0))).to(a)\n",
    "        yield rewrite(shrl(a, Num(0))).to(a)\n",
    "        yield rewrite(not_(not_(a))).to(a)\n",
    "    \n",
    "        # Additional rules from your snake-egg implementation\n",
    "        yield rewrite(mul(mul(a, b), c)).to(mul(b, mul(a, c)))  # Multiplicative associativity\n",
    "        yield rewrite(sub(a, b)).to(add(a, neg_(b)))\n",
    "        yield rewrite(mul(a, Num(2))).to(shll(a, Num(1)))\n",
    "        yield rewrite(shll(shll(a, b), c)).to(shll(a, add(b, c)))\n",
    "        yield rewrite(shrl(shrl(a, b), c)).to(shrl(a, add(b, c)))\n",
    "        yield rewrite(neg_(a)).to(add(not_(a), Num(1)))\n",
    "        yield rewrite(add(a, a)).to(mul(a, Num(2)))\n",
    "        yield rewrite(add(mul(a, b), b)).to(mul(add(a, Num(1)), b))\n",
    "    \n",
    "        # Constant folding\n",
    "        yield rewrite(add(Num(i), Num(j))).to(Num(i + j))\n",
    "        yield rewrite(mul(Num(i), Num(j))).to(Num(i * j))\n",
    "        yield rewrite(sub(Num(i), Num(j))).to(Num(i - j))\n",
    "        yield rewrite(div(Num(i), Num(j))).to(Num(i / j))\n",
    "        yield rewrite(shll(Num(i), Num(j))).to(Num(i << j))\n",
    "        yield rewrite(shrl(Num(i), Num(j))).to(Num(i >> j))\n",
    "        yield rewrite(not_(Num(i))).to(Num(~i))\n",
    "    # yield rewrite(neg_(Num(i))).to(Num(-i))\n",
    "\n",
    "# def get_egraph_in_range(expr, max_iters=5):\n",
    "#     egraph.saturate()\n",
    "#     return egraph.extract_multiple(expr, max_iters)\n",
    "\n",
    "    \n",
    "    expr_id = egraph.let(\"expr\", eval(expr))\n",
    "    egraph.run(max_iters)\n",
    "    return egraph.extract_multiple(expr_id, max_iters)\n",
    "\n",
    "# Create an expression and add it to the e-graph\n",
    "# expr = add(Num.var(\"c\"), add(Num.var(\"a\"), Num.var(\"b\")))\n",
    "expr = 'add(Num.var(\"a\"), Num.var(\"a\"))'\n",
    "\n",
    "# Extract multiple forms of the expression\n",
    "get_egraph_in_range(expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_expression(expr):\n",
    "    # Convert the expression to a string\n",
    "    expr_str = str(expr)\n",
    "\n",
    "    # Function to replace Num.var( and Num( with empty strings and remove one corresponding closing parenthesis\n",
    "    def replace_with_balanced_paren(match):\n",
    "        # Remove 'Num.var(' or 'Num('\n",
    "        modified_str = match.group(0).replace('Num.var(', '', 1).replace('Num(', '', 1)\n",
    "        # Find the position of the corresponding closing parenthesis\n",
    "        open_count = 0\n",
    "        for i, char in enumerate(modified_str):\n",
    "            if char == '(':\n",
    "                open_count += 1\n",
    "            elif char == ')':\n",
    "                if open_count == 0:\n",
    "                    # This is the closing parenthesis to remove\n",
    "                    modified_str = modified_str[:i] + modified_str[i+1:]\n",
    "                    break\n",
    "                else:\n",
    "                    open_count -= 1\n",
    "        return modified_str\n",
    "\n",
    "    # Apply the replacement function to the expression string\n",
    "    expr_str = re.sub(r'Num\\.var\\([^\\)]+\\)|Num\\([^\\)]+\\)', replace_with_balanced_paren, expr_str)\n",
    "    expr_str = expr_str.replace('\"',\"'\").replace(\" \",\"\")\n",
    "    # print(\"Gen This: \", expr_str)\n",
    "    return expr_str\n",
    "\n",
    "# Example usage\n",
    "# egglog_expr = add(Num.var(\"c\"), add(Num.var(\"a\"), Num.var(\"b\")))\n",
    "# simplified_expr = simplify_expression(egglog_expr)\n",
    "# print(simplified_expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json_from_ir(ir_input_path, json_output_path, rewrite_rule = \"NoRule\"):\n",
    "    # This function takes in ir file and generate json instruction based on e-graph\n",
    "    \n",
    "    TopFunctionName = None\n",
    "    with open(ir_input_path, 'r') as f:\n",
    "        ir_content = f.read()\n",
    "        ir_dict = slice_ir_by_function(ir_content)\n",
    "\n",
    "    #     print(ir_content)\n",
    "\n",
    "    FuncNodeDict = {}\n",
    "    for fn_name, fn_content in ir_dict.items():\n",
    "        NodeDict = {}\n",
    "        Lines = ParseIRFile(fn_content)\n",
    "        for Line in Lines:    \n",
    "            NodeList = LineToDict(Line)\n",
    "            if NodeList != None:\n",
    "                for NodeObj in NodeList:\n",
    "                    NodeDict[NodeObj.name] = NodeObj\n",
    "            if \"top\" in Line:\n",
    "                TopFunctionName = fn_name\n",
    "        FuncNodeDict[fn_name] = NodeDict\n",
    "    SubCounter = 0\n",
    "    NodeCounter = 0\n",
    "    JsonOutDict = {}\n",
    "#     print(FuncNodeDict)\n",
    "    if rewrite_rule == \"only_func_name\":\n",
    "        return None, TopFunctionName\n",
    "\n",
    "    ## The following code reads digraph and partitions them into legal rewrite batches\n",
    "    graph_by_function = {}\n",
    "    sorted_nodes_by_function = {}\n",
    "\n",
    "    for fn_name, fn_nodes in FuncNodeDict.items():\n",
    "        G = nx.DiGraph()\n",
    "        G = DictToGraph(G, fn_nodes)\n",
    "        graph_by_function[fn_name] = G\n",
    "        \n",
    "#         nx.draw(G, with_labels=True, font_weight='bold')\n",
    "\n",
    "        # Check if the graph is a DAG\n",
    "        if nx.is_directed_acyclic_graph(G):\n",
    "            sorted_nodes = list(nx.topological_sort(G))\n",
    "            sorted_nodes_by_function[fn_name] = sorted_nodes[::-1]\n",
    "        else:\n",
    "            sorted_nodes_by_function[fn_name] = None\n",
    "            print(f\"[Warning] Graph for function {fn_name} is not a DAG. Topological sorting cannot be performed.\")\n",
    "\n",
    "        # print(f\"Function: {fn_name}\")\n",
    "        # print(\"Batches of nodes:\", sorted_nodes_by_function[fn_name])   \n",
    "\n",
    "\n",
    "    rewrite_batch_by_function = {}\n",
    "    processed_nodes = set()\n",
    "\n",
    "    for fn_name, nodes in sorted_nodes_by_function.items():\n",
    "        G = graph_by_function[fn_name]\n",
    "        rewrite_batch_by_function[fn_name] = []\n",
    "\n",
    "        temp_nodes = nodes.copy()\n",
    "        cur_index = 0\n",
    "        while temp_nodes and cur_index < len(temp_nodes):\n",
    "            start_node = temp_nodes[cur_index]\n",
    "            cur_batch = traverse_graph(G, start_node, processed_nodes)\n",
    "#             print(\"Current  Batch: \", cur_batch)\n",
    "            if cur_batch != []:\n",
    "                rewrite_batch_by_function[fn_name].append(cur_batch)\n",
    "                for node in cur_batch:\n",
    "                    processed_nodes.add(node)\n",
    "                    if node in temp_nodes:\n",
    "                        temp_nodes.remove(node)\n",
    "            else:\n",
    "                cur_index += 1\n",
    "\n",
    "        # print(f\"Function: {fn_name}\")\n",
    "        # print(\"Batches of nodes:\", rewrite_batch_by_function[fn_name])\n",
    "\n",
    "    ## The following code takes each batch, and generate expression for e-graph search\n",
    "    rewrite_candidate_by_function = {}\n",
    "    for fn_name, batch_list in rewrite_batch_by_function.items():\n",
    "        G = graph_by_function[fn_name]\n",
    "        rewrite_candidate_by_function[fn_name] = {}\n",
    "        rewrite_candidate_by_function[fn_name]['is_unsigned'] = []\n",
    "        rewrite_candidate_by_function[fn_name]['rewrite_candidates'] = []\n",
    "        for batch in batch_list:\n",
    "            batch_expr, is_unsigned = get_expr_from_batch(G, batch)\n",
    "            # print(\"FunctionName: \", fn_name)\n",
    "            # print(\"Input Expr: \", batch_expr)\n",
    "            cur_candidates = get_egraph_in_range(batch_expr)\n",
    "            rewrite_candidate_by_function[fn_name]['is_unsigned'].append(is_unsigned)\n",
    "            rewrite_candidate_by_function[fn_name]['rewrite_candidates'] .append(cur_candidates)\n",
    "            # print(cur_candidates)\n",
    "    # print(rewrite_candidate_by_function)\n",
    "\n",
    "    operation_mapping_unsigned = {\"add\" : \"kAdd\",\n",
    "                                  \"mul\" : \"kUMul\",\n",
    "                                  \"div\" : \"kUDiv\",\n",
    "                                  \"sub\" : \"kSub\",\n",
    "                                \"literal\" : \"Literal\",\n",
    "                                 \"neg_\" : \"kNeg\",\n",
    "                                 \"shll\" : \"kShll\",\n",
    "                                 \"shrl\" : \"kShrl\",\n",
    "                                 \"not_\" : \"kNot\"}\n",
    "\n",
    "    operation_mapping_signed = {\"add\" : \"kAdd\",\n",
    "                                  \"mul\" : \"kSMul\",\n",
    "                                  \"div\" : \"kSDiv\",\n",
    "                                  \"sub\" : \"kSub\",\n",
    "                                \"literal\" : \"Literal\",\n",
    "                                 \"neg_\" : \"kNeg\",\n",
    "                                 \"shll\" : \"kShll\",\n",
    "                                 \"shrl\" : \"kShrl\",\n",
    "                                 \"not_\" : \"kNot\"}\n",
    "    json_dict = {}\n",
    "    counter = 0\n",
    "    for fn_name, all_rewrite_candidates_dict in rewrite_candidate_by_function.items():\n",
    "        G = graph_by_function[fn_name]\n",
    "        for i in range(len(all_rewrite_candidates_dict['rewrite_candidates'])):\n",
    "            cur_rewrite_candidate = all_rewrite_candidates_dict['rewrite_candidates'][i]\n",
    "            \n",
    "            #First we initialize the dictionary\n",
    "            json_dict[str(counter)] = {}\n",
    "            json_dict[str(counter)][\"FuncName\"] = fn_name\n",
    "\n",
    "            #Then, find bitwidth and if the variables are signed, and assign operator mapping\n",
    "            cur_is_unsigned = all_rewrite_candidates_dict['is_unsigned'][i]\n",
    "            bit_width = 0\n",
    "            for cur_node_name in rewrite_batch_by_function[fn_name][i]:\n",
    "                cur_bit_width = G.nodes[cur_node_name]['BitWidth']\n",
    "                if cur_bit_width > bit_width:\n",
    "                    bit_width = cur_bit_width\n",
    "                    \n",
    "            if cur_is_unsigned:\n",
    "                cur_operation_mapping = operation_mapping_unsigned\n",
    "            else:\n",
    "                cur_operation_mapping = operation_mapping_signed\n",
    "\n",
    "            # Change the formatting of the exprs\n",
    "            simplied_rewrite_candidate = []\n",
    "            for expr in cur_rewrite_candidate:\n",
    "                simplied_rewrite_candidate.append(simplify_expression(expr))\n",
    "            cur_rewrite_candidate = simplied_rewrite_candidate\n",
    "            \n",
    "            # Now we call our selector to get a best rewrite\n",
    "            if rewrite_rule == \"NoRule\":\n",
    "                rewrite_expr = cur_rewrite_candidate[-1]\n",
    "            elif rewrite_rule == \"NaivePick\":\n",
    "                rewrite_expr = naive_pick(G, cur_rewrite_candidate, bit_width, cur_operation_mapping)\n",
    "            else:\n",
    "                rewrite_expr = cur_rewrite_candidate[-1]  \n",
    "\n",
    "            \n",
    "#             print(\"FunctionName: \",fn_name)\n",
    "            # print(\"Rewrite Expr\", rewrite_expr)\n",
    "            \n",
    "            #Next, handle node generation\n",
    "            nodes_involved_dict, out_node_name = gen_json_from_expr_recursive(G, str(rewrite_expr), cur_operation_mapping, bit_width)\n",
    "\n",
    "            #Next, handle node elimination\n",
    "            for old_node_id in rewrite_batch_by_function[fn_name][i]:\n",
    "                nodes_involved_dict[G.nodes[old_node_id]['OperationName']] = G.nodes[old_node_id]\n",
    "                nodes_involved_dict[G.nodes[old_node_id]['OperationName']]['ReplaceSelfWith'] = 'Kill'\n",
    "\n",
    "            #Next, handle batch output replacement\n",
    "            #ToDo: we need to verify the first is always the output of the batch\n",
    "            nodes_involved_dict[G.nodes[rewrite_batch_by_function[fn_name][i][0]]['OperationName']] = G.nodes[rewrite_batch_by_function[fn_name][i][0]]\n",
    "            nodes_involved_dict[G.nodes[rewrite_batch_by_function[fn_name][i][0]]['OperationName']]['ReplaceSelfWith'] = str(out_node_name)\n",
    "\n",
    "            json_dict[str(counter)][\"NodesInvolved\"] = list(nodes_involved_dict.values())\n",
    "            counter += 1\n",
    "            \n",
    "#             print(nodes_involved_dict)\n",
    "\n",
    "    with open(json_output_path, 'w') as json_file:\n",
    "            json.dump(json_dict, json_file, indent=4)  \n",
    "    return G, TopFunctionName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are helper functions for selectors\n",
    "def expr_to_graph(G, expr, bit_width, cur_operation_mapping):\n",
    "    cur_graph = nx.DiGraph()\n",
    "    nodes_involved_dict, out_node_name = gen_json_from_expr_recursive(G, str(expr), cur_operation_mapping, bit_width)\n",
    "    filtered_dict = {k: v for k, v in nodes_involved_dict.items() if v['ReplaceSelfWith'] == 'Gen'}\n",
    "    node_obj_dict = dict_to_nodes(filtered_dict)\n",
    "#     print(node_obj_dict, \"\\n\")\n",
    "    node_name_list = list(node_obj_dict.keys())\n",
    "    new_nodes = {}\n",
    "    # Iterate over the nodes and check for missing operands\n",
    "    for node_name, node_obj in node_obj_dict.items():\n",
    "        if node_obj.operands != None:\n",
    "            for operand in node_obj.operands:\n",
    "                if operand not in node_name_list and operand not in new_nodes:\n",
    "                    # Create a new Node object for the missing operand\n",
    "                    new_nodes[operand] = Node(name=operand, operands=[])\n",
    "        else:\n",
    "            node_obj.operands = []\n",
    "        \n",
    "    # Merge the new nodes into the main dictionary\n",
    "    node_obj_dict.update(new_nodes)\n",
    "    counter = 0\n",
    "    for node_name, node_obj in node_obj_dict.items():\n",
    "        node_obj.idNum = counter\n",
    "        counter += 1\n",
    "    cur_graph = DictToGraph(cur_graph, node_obj_dict)\n",
    "#     print(\"expr: \", expr)\n",
    "#     print(\"graph\", cur_graph.nodes(), \"\\n\")\n",
    "    return cur_graph\n",
    "\n",
    "def dict_to_nodes(node_dict):\n",
    "    inversed_mapping = {\"kAdd\" : \"add\",\n",
    "                          \"kUMul\" : \"mul\",\n",
    "                        \"kSMul\" : \"mul\",\n",
    "                          \"kUDiv\" : \"div\",\n",
    "                        \"kSDiv\" : \"div\",\n",
    "                          \"kSub\" : \"sub\",\n",
    "                        \"Literal\" : \"literal\",\n",
    "                         \"kNeg\" : \"neg\",\n",
    "                         \"kShll\" : \"shll\",\n",
    "                         \"kShrl\" : \"shrl\",\n",
    "                         \"kNot\" : \"not_\",\n",
    "                       \"kConcat\" : \"concat\",\n",
    "                       \"bit_slice\" : \"bitslice\"}\n",
    "    node_objects = {}    \n",
    "    for node_name, node_data in node_dict.items():\n",
    "        if node_data.get(\"OperationType\") in inversed_mapping.keys():\n",
    "            mapped_operation = inversed_mapping[node_data.get(\"OperationType\")]\n",
    "        else:\n",
    "            mapped_operation = node_data.get(\"OperationType\")\n",
    "        # Create Node object using the data from the dictionary\n",
    "        node = Node(\n",
    "            name=node_data.get(\"OperationName\"),\n",
    "            bitwidth=node_data.get(\"BitWidth\"),\n",
    "            operation=mapped_operation,\n",
    "            operands=node_data.get(\"Operands\"),\n",
    "            idNum=node_data.get(\"idNum\"),\n",
    "            value=node_data.get(\"Value\"),\n",
    "            pos=node_data.get(\"Pos\"),\n",
    "            FuncIO=node_data.get(\"FuncIO\"),\n",
    "            start=node_data.get(\"Start\"),\n",
    "            width=node_data.get(\"Width\"),\n",
    "            array_sizes=node_data.get(\"ArraySize\"),\n",
    "            indices=node_data.get(\"Indices\")\n",
    "        )\n",
    "        # Store the Node object using the node name as the key\n",
    "        node_objects[node_name] = node\n",
    "    return node_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_with_smallest_longest_path(graph_list):\n",
    "    min_longest_path_length = float('inf')\n",
    "    index_of_min = -1\n",
    "\n",
    "    for i, graph in enumerate(graph_list):\n",
    "        try:\n",
    "            # Assuming the graph is a DAG\n",
    "            longest_path_length = len(nx.dag_longest_path(graph)) - 1  # Subtract 1 to get the number of edges\n",
    "            if longest_path_length < min_longest_path_length:\n",
    "                min_longest_path_length = longest_path_length\n",
    "                index_of_min = i\n",
    "            elif longest_path_length == min_longest_path_length:\n",
    "                # Update the index if it's larger in case of tie\n",
    "                index_of_min = max(index_of_min, i)\n",
    "        except nx.NetworkXUnfeasible:\n",
    "            # This exception is raised if the graph is not a DAG\n",
    "            print(f\"Graph at index {i} is not a DAG.\")\n",
    "\n",
    "    return index_of_min\n",
    "\n",
    "def get_graph_with_fewest_nodes(graph_list):\n",
    "    min_node_count = float('inf')\n",
    "    index_of_min = -1\n",
    "\n",
    "    for i, graph in enumerate(graph_list):\n",
    "        node_count = len(graph.nodes)\n",
    "\n",
    "        if node_count < min_node_count:\n",
    "            min_node_count = node_count\n",
    "            index_of_min = i\n",
    "        elif node_count == min_node_count:\n",
    "            # Update the index if it's larger in case of tie\n",
    "            index_of_min = max(index_of_min, i)\n",
    "#         print(node_count)\n",
    "#     print(\"Chose: \", index_of_min)\n",
    "    return index_of_min\n",
    "\n",
    "\n",
    "def naive_pick(G, cur_rewrite_candidate, bit_width, cur_operation_mapping):\n",
    "    graph_list = []\n",
    "    for cur_expr in cur_rewrite_candidate:\n",
    "        cur_graph = expr_to_graph(G, cur_expr,  bit_width, cur_operation_mapping)\n",
    "        graph_list.append(cur_graph)  \n",
    "#         print(cur_expr)\n",
    "#         print(cur_graph.nodes)\n",
    "\n",
    "#     native_pick_index = get_graph_with_smallest_longest_path(graph_list)\n",
    "    native_pick_index = get_graph_with_fewest_nodes(graph_list)\n",
    "    best_rewrite = cur_rewrite_candidate[native_pick_index]\n",
    "#     print(native_pick_index)\n",
    "#     print(\"\\n\")\n",
    "    return best_rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sdc_from_graph(ir_input_path, delay_model='sky130', \n",
    "               Schedule_Method=\"SDC\", selector=\"only_func_name\", clock_period_ps=None, clock_margin_precent=None, \n",
    "               pipeline_stages=None, period_relaxation_percent=None):\n",
    "    \n",
    "    command_executable = '/home/xlsrl/XLSRL/bazel-bin/xls/tools/RL_main'\n",
    "    ir_unify_name_out = ir_input_path.replace('.ir','.unify.ir')\n",
    "    json_output_path = ir_input_path.replace('.ir','.json')\n",
    "    ir_output_path = ir_input_path.replace('.ir','_substitution.ir')\n",
    "    schedule_result_path = ir_input_path.replace('.ir', '_schedule.txt')\n",
    "    output_verilog_path = json_output_path.replace(\".json\", \".v\")\n",
    "    output_schedule_ir_path = ir_output_path.replace(\".ir\", \"_schedule.ir\")\n",
    "    \n",
    "    run_unify_name(ir_input_path, ir_unify_name_out)\n",
    "    #Get rewrite json file from IR and selector model\n",
    "    G, TopFunctionName = gen_json_from_ir(ir_unify_name_out, json_output_path, rewrite_rule=\"only_func_name\")\n",
    "    #Run cc rewriter to implement the rewrite\n",
    "#     run_rewriter(command_executable, ir_unify_name_out, json_output_path, ir_output_path)\n",
    "    #Run SDC with the new IR and generate result.\n",
    "    run_sdc_scheduler(ir_unify_name_out, output_verilog_path, schedule_result_path, output_schedule_ir_path, \n",
    "                      delay_model, TopFunctionName, clock_period_ps, clock_margin_precent, \n",
    "                      pipeline_stages, period_relaxation_percent)\n",
    "    #Collect the scheduling result to a graph\n",
    "    G_schedule, max_stage_latency, stage_num, register_count = get_graph_with_scheduling(output_schedule_ir_path, schedule_result_path)\n",
    "    print(\"Original Latency: \", max_stage_latency, \"stage num: \", stage_num, \"register count: \", register_count)\n",
    "    return G_schedule, max_stage_latency, stage_num, register_count\n",
    "\n",
    "\n",
    "def egraph_flow(ir_input_path, delay_model='sky130', \n",
    "               Schedule_Method=\"SDC\", selector=\"NoRule\", clock_period_ps=None, clock_margin_precent=None, \n",
    "               pipeline_stages=None, period_relaxation_percent=None):\n",
    "    \n",
    "    command_executable = '/home/xlsrl/XLSRL/bazel-bin/xls/tools/RL_main'\n",
    "    ir_unify_name_out = ir_input_path.replace('.ir','.unify.ir')\n",
    "    json_output_path = ir_input_path.replace('.ir','.json')\n",
    "    ir_output_path = ir_input_path.replace('.ir','_substitution.ir')\n",
    "    schedule_result_path = ir_input_path.replace('.ir', '_schedule.txt')\n",
    "    output_verilog_path = json_output_path.replace(\".json\", \".v\")\n",
    "    output_schedule_ir_path = ir_output_path.replace(\".ir\", \"_schedule.ir\")\n",
    "    \n",
    "    run_unify_name(ir_input_path, ir_unify_name_out)\n",
    "    #Get rewrite json file from IR and selector model\n",
    "    G, TopFunctionName = gen_json_from_ir(ir_unify_name_out, json_output_path, rewrite_rule=selector)\n",
    "    #Run cc rewriter to implement the rewrite\n",
    "    run_rewriter(command_executable, ir_unify_name_out, json_output_path, ir_output_path)\n",
    "    #Run SDC with the new IR and generate result.\n",
    "    run_sdc_scheduler(ir_output_path, output_verilog_path, schedule_result_path, output_schedule_ir_path, \n",
    "                      delay_model, TopFunctionName, clock_period_ps, clock_margin_precent, \n",
    "                      pipeline_stages, period_relaxation_percent)\n",
    "    #Collect the scheduling result to a graph\n",
    "    G_schedule, max_stage_latency, stage_num, register_count = get_graph_with_scheduling(output_schedule_ir_path, schedule_result_path)\n",
    "    print(\"Original Latency: \", max_stage_latency, \"stage num: \", stage_num, \"register count: \", register_count)\n",
    "    return G_schedule, max_stage_latency, stage_num, register_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run_unify_name(\"/home/xlsrl/XLSRL/work_space/Sha256/sha256.opt.ir\", \"/home/xlsrl/XLSRL/work_space/Sha256/sha256.opt.unify.ir\")\n",
    "# G, FuncName = gen_json_from_ir(\"/home/xlsrl/XLSRL/work_space/Sha256/sha256.opt.unify.ir\", \"/home/xlsrl/XLSRL/work_space/Sha256/sha256.json\", rewrite_rule=\"NativePick\")\n",
    "# nx.draw(G, with_labels=True, font_weight='bold')\n",
    "# G, FuncName = gen_json_from_ir(\"/home/xlsrl/XLSRL/work_space/EGraphUnitTest/all_unit_test.ir\", \"/home/xlsrl/XLSRL/work_space/EGraphUnitTest/all_unit_test.json\", rewrite_rule=\"NativePick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name Unification Run Done.\n",
      "\n",
      "Running Standalone Rewriter\n",
      "An error occurred while running the command.\n",
      "Error message: Error: INVALID_ARGUMENT: One or more nodes cannot be generated due to dependency. Details:\n",
      "Key: auto_gen5 Value: {Value: -1, OperationName: auto_gen5, OperationType: kAdd, Id: 64, ReplaceSelfWith: Gen, BitWidth: 32, Position: [], Operands: [auto_gen1, auto_gen4, ], Indices: [], ArraySize: [] }\n",
      "\n",
      "An error occurred while running the command.\n",
      "Error message: Error: NOT_FOUND: No such file or directory; /home/xlsrl/XLSRL/work_space/Sha256/sha256.opt_substitution.ir\n",
      "Reading Scheduling Result\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/xlsrl/XLSRL/work_space/Sha256/sha256.opt_substitution_schedule.ir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# egraph_flow(\"/home/xlsrl/XLSRL/work_space/EGraphTest/test.opt.ir\", clock_period_ps = 1000)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# egraph_flow(\"/home/xlsrl/XLSRL/work_space/EGraphUnitTest/all_unit_test.ir\", clock_period_ps = 1000)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# get_sdc_from_graph(\"/home/xlsrl/XLSRL/work_space/Sha256/sha256.opt.ir\", clock_period_ps = 3000)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43megraph_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/xlsrl/XLSRL/work_space/Sha256/sha256.opt.ir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclock_period_ps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNaivePick\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# egraph_flow(\"/home/xlsrl/XLSRL/work_space/adler32/adler32.opt.ir\", clock_period_ps = 25825)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[62], line 50\u001b[0m, in \u001b[0;36megraph_flow\u001b[0;34m(ir_input_path, delay_model, Schedule_Method, selector, clock_period_ps, clock_margin_precent, pipeline_stages, period_relaxation_percent)\u001b[0m\n\u001b[1;32m     46\u001b[0m run_sdc_scheduler(ir_output_path, output_verilog_path, schedule_result_path, output_schedule_ir_path, \n\u001b[1;32m     47\u001b[0m                   delay_model, TopFunctionName, clock_period_ps, clock_margin_precent, \n\u001b[1;32m     48\u001b[0m                   pipeline_stages, period_relaxation_percent)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m#Collect the scheduling result to a graph\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m G_schedule, max_stage_latency, stage_num, register_count \u001b[38;5;241m=\u001b[39m \u001b[43mget_graph_with_scheduling\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_schedule_ir_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschedule_result_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Latency: \u001b[39m\u001b[38;5;124m\"\u001b[39m, max_stage_latency, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage num: \u001b[39m\u001b[38;5;124m\"\u001b[39m, stage_num, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister count: \u001b[39m\u001b[38;5;124m\"\u001b[39m, register_count)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m G_schedule, max_stage_latency, stage_num, register_count\n",
      "Cell \u001b[0;32mIn[55], line 66\u001b[0m, in \u001b[0;36mget_graph_with_scheduling\u001b[0;34m(output_schedule_ir_path, schedule_result_path)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_graph_with_scheduling\u001b[39m(output_schedule_ir_path, schedule_result_path):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading Scheduling Result\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m     G_schedule, register_count \u001b[38;5;241m=\u001b[39m \u001b[43mReadScheduleIR\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_schedule_ir_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     schedule_dict, max_stage_latency \u001b[38;5;241m=\u001b[39m read_SDC_pipeline_result(schedule_result_path)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#     print(schedule_dict)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[54], line 24\u001b[0m, in \u001b[0;36mReadScheduleIR\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mReadScheduleIR\u001b[39m(file_path):\n\u001b[1;32m     23\u001b[0m     TopFunctionName \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     25\u001b[0m         ir_content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     26\u001b[0m         ir_dict \u001b[38;5;241m=\u001b[39m slice_ir_by_function(ir_content)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/xlsrl/XLSRL/work_space/Sha256/sha256.opt_substitution_schedule.ir'"
     ]
    }
   ],
   "source": [
    "# egraph_flow(\"/home/xlsrl/XLSRL/work_space/EGraphTest/test.opt.ir\", clock_period_ps = 1000)\n",
    "# egraph_flow(\"/home/xlsrl/XLSRL/work_space/EGraphUnitTest/all_unit_test.ir\", clock_period_ps = 1000)\n",
    "# get_sdc_from_graph(\"/home/xlsrl/XLSRL/work_space/Sha256/sha256.opt.ir\", clock_period_ps = 3000)\n",
    "egraph_flow(\"/home/xlsrl/XLSRL/work_space/Sha256/sha256.opt.ir\", clock_period_ps = 3000, selector=\"NaivePick\")\n",
    "# egraph_flow(\"/home/xlsrl/XLSRL/work_space/adler32/adler32.opt.ir\", clock_period_ps = 25825)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (egglog)",
   "language": "python",
   "name": "egglog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
