{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, List\n",
    "\n",
    "from collections import namedtuple\n",
    "from snake_egg import EGraph, Rewrite, Var, vars\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import isomorphism\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, name=None, bitwidth=None, operation=None, operands=None, \n",
    "                 idNum=None, value=-1, pos=None, FuncIO=None, start=None, width=None, array_sizes=None, indices=None):\n",
    "        self.name = name\n",
    "        self.bitwidth = bitwidth\n",
    "        self.operation = operation\n",
    "        self.operands = operands\n",
    "        self.idNum = idNum\n",
    "        self.value = value\n",
    "        self.pos = pos\n",
    "        self.FuncIO = FuncIO\n",
    "        self.start = start\n",
    "        self.width = width\n",
    "        self.array_sizes = array_sizes\n",
    "        self.indices = indices\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Node(name={self.name}, bitwidth={self.bitwidth}, operation={self.operation}, operands={self.operands}, id={self.idNum}, value={self.value}, pos={self.pos}, FuncIO={self.FuncIO}, start={self.start}, width={self.width}, array_sizes={self.array_sizes}, indices={self.indices})\"\n",
    "\n",
    "def node_to_dict(node):\n",
    "    return {\n",
    "        \"OperationName\": node.name,\n",
    "        \"BitWidth\": node.bitwidth,\n",
    "        \"OperationType\": node.operation,\n",
    "        \"Operands\": node.operands,\n",
    "        \"idNum\": node.idNum,\n",
    "        \"Value\": node.value,\n",
    "        \"Pos\": node.pos,\n",
    "        \"FuncIO\": node.FuncIO,\n",
    "        \"Start\": node.start,\n",
    "        \"Width\": node.width,\n",
    "        \"ArraySize\": node.array_sizes,\n",
    "        \"Indices\": node.indices\n",
    "    }\n",
    "    \n",
    "def ParseIRFile(ir_text):\n",
    "    nodes_dict = {}\n",
    "    lines = ir_text.strip().split(\"\\n\")[:]\n",
    "    return lines\n",
    "\n",
    "def LineToDict(line):\n",
    "#     print(line)\n",
    "    nodes = []\n",
    "    # Check for function header\n",
    "    FuncIO = \"Not\"\n",
    "    if \"fn\" in line:\n",
    "        FuncIO = \"In\"\n",
    "        signature_pattern = re.compile(r\"fn\\s+(\\w+)\\((.*?)\\)\\s*->\\s*(.*?)\\s*\\{\")\n",
    "        signature_match = signature_pattern.search(line)\n",
    "        if signature_match:\n",
    "            func_name = signature_match.group(1)\n",
    "            params = signature_match.group(2).split(',')\n",
    "            for param in params:\n",
    "                param_name, param_type = param.split(':')\n",
    "                param_name = param_name.strip()\n",
    "                array_sizes = [int(size) for size in re.findall(r'\\[(\\d+)\\]', param_type)]\n",
    "                if array_sizes == []:\n",
    "                    array_sizes = None\n",
    "                bitwidth = int(array_sizes.pop(0))  # The first size is the bitwidth\n",
    "                node = Node(name=param_name, bitwidth=bitwidth, array_sizes=array_sizes, FuncIO=\"In\")\n",
    "                nodes.append(node)\n",
    "        return nodes\n",
    "\n",
    "    if \"ret\" in line:\n",
    "        FuncIO = \"Out\"\n",
    "        line = line[4:]\n",
    "        \n",
    "    if line == \"}\":\n",
    "        return None\n",
    "\n",
    "    # Extract node name\n",
    "#     name = re.search(r\"(\\w+\\.\\d+):\", line).group(1)\n",
    "    name = re.search(r\"(\\w+(\\.\\d+)?)\\:\", line).group(1)\n",
    "\n",
    "    # Extract bitwidth and array size\n",
    "    array_pattern = re.compile(r\"bits\\[(\\d+)\\]((?:\\[\\d+\\])*)\")\n",
    "    array_match = array_pattern.search(line)\n",
    "    if array_match:\n",
    "        bitwidth = int(array_match.group(1))\n",
    "        array_sizes_str = array_match.group(2)\n",
    "        array_sizes = [int(size) for size in re.findall(r'\\[(\\d+)\\]', array_sizes_str)]\n",
    "        if array_sizes == []:\n",
    "            array_sizes = None\n",
    "    \n",
    "    # Extract operation\n",
    "    operation = re.search(r\"= (\\w+)\", line).group(1)\n",
    "\n",
    "    # Extract operands; \n",
    "    # Eidt to ignore other argument keywords\n",
    "    operands = re.findall(r\"(\\w+\\.\\d+|\\w+)\", line.split(\"=\")[1])\n",
    "    operands = [op for op in operands if op not in [operation, \"value\", \"id\", \"pos\", \"start\", \"width\", \"indices\"]]\n",
    "\n",
    "    # Extract indices: only applies for array_indice node\n",
    "    indices = None\n",
    "    indices_match = re.search(r\"indices=\\[(.*?)\\]\", line)\n",
    "    if indices_match:\n",
    "        # Get the string of indices\n",
    "        indices_str = indices_match.group(1)\n",
    "        # Find all occurrences of literals or node names in the indices string\n",
    "        indices = re.findall(r\"(\\w+\\.\\d+|\\w+)\", indices_str)\n",
    "    \n",
    "    # Extract id\n",
    "    idNum = int(re.search(r\"id=(\\d+)\", line).group(1))\n",
    "\n",
    "    # Extract value (if present)\n",
    "    value_match = re.search(r\"value=(\\d+)\", line)\n",
    "    value = int(value_match.group(1)) if value_match else -1\n",
    "\n",
    "    # Extract pos (if present)\n",
    "    pos_match = re.search(r\"pos=\\[\\((\\d+,\\d+,\\d+)\\)\\]\", line)\n",
    "    pos = tuple(map(int, pos_match.group(1).split(\",\"))) if pos_match else None\n",
    "\n",
    "    # Extract start and width for bit_slice (if present)\n",
    "    start = None\n",
    "    width = None\n",
    "    if operation == \"bit_slice\":\n",
    "        start = int(re.search(r\"start=(\\d+)\", line).group(1))\n",
    "        width = int(re.search(r\"width=(\\d+)\", line).group(1))\n",
    "\n",
    "    # Create Node object\n",
    "    node = Node(name, bitwidth, operation, operands, idNum, value, pos, FuncIO, start, width, array_sizes, indices)\n",
    "    \n",
    "    # Add to node list\n",
    "    nodes.append(node)\n",
    "    return nodes\n",
    "\n",
    "def DictToGraph(G, NodeDict):\n",
    "    # This function turns the node dictionary to directional graph\n",
    "    NodeList = []\n",
    "    EdgeList = []\n",
    "    for NodeName in NodeDict:\n",
    "        if NodeDict[NodeName].FuncIO != \"In\": \n",
    "            # This if is for handling the function top input, they are treated as nodes as well.\n",
    "            NodeList.append((NodeDict[NodeName].idNum, node_to_dict(NodeDict[NodeName])))\n",
    "            for ParentName in NodeDict[NodeName].operands:\n",
    "                if NodeDict[ParentName].FuncIO != \"In\":\n",
    "                    EdgeList.append((NodeDict[ParentName].idNum, NodeDict[NodeName].idNum))\n",
    "                else: \n",
    "                    EdgeList.append((NodeDict[ParentName].name, NodeDict[NodeName].idNum))\n",
    "            if NodeDict[NodeName].indices != None:\n",
    "                for ParentName in NodeDict[NodeName].indices:\n",
    "                    if NodeDict[ParentName].FuncIO != \"In\":\n",
    "                        EdgeList.append((NodeDict[ParentName].idNum, NodeDict[NodeName].idNum))\n",
    "                    else: \n",
    "                        EdgeList.append((NodeDict[ParentName].name, NodeDict[NodeName].idNum))\n",
    "        else:\n",
    "            NodeList.append((NodeDict[NodeName].name, node_to_dict(NodeDict[NodeName])))\n",
    "    G.add_nodes_from(NodeList)\n",
    "    G.add_edges_from(EdgeList)\n",
    "    return G\n",
    "    \n",
    "    \n",
    "def slice_ir_by_function(ir_content):\n",
    "    # Regular expression to match function definitions with and without \"top\"\n",
    "    fn_pattern = re.compile(r\"(?:top\\s+)?fn\\s+(\\w+)\\((.*?)\\)\\s*->\\s*(.*?)\\s*\\{\")\n",
    "\n",
    "    # Split the content by lines for processing\n",
    "    lines = ir_content.split('\\n')\n",
    "\n",
    "    # Dictionary to store each function's IR content\n",
    "    functions_dict = {}\n",
    "\n",
    "    # Buffer to store current function lines\n",
    "    current_fn = None\n",
    "    current_fn_lines = []\n",
    "\n",
    "    # Iterate through each line\n",
    "    for line in lines:\n",
    "        # Check if the line starts a new function definition\n",
    "        match = fn_pattern.match(line)\n",
    "        if match:\n",
    "            # If there is a current function being processed, save it\n",
    "            if current_fn:\n",
    "                functions_dict[current_fn] = '\\n'.join(current_fn_lines)\n",
    "                current_fn_lines = []\n",
    "            # Start a new function\n",
    "            current_fn = match.group(1)\n",
    "        # Add the line to the current function's lines\n",
    "        if current_fn:\n",
    "            current_fn_lines.append(line)\n",
    "        # Check if the line ends a function definition\n",
    "        if line.strip() == '}':\n",
    "            # Save the current function and reset\n",
    "            if current_fn:\n",
    "                functions_dict[current_fn] = '\\n'.join(current_fn_lines)\n",
    "                current_fn = None\n",
    "                current_fn_lines = []\n",
    "\n",
    "    # Return the dictionary with function names as keys and IR content as values\n",
    "    return functions_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadScheduleIR(file_path):\n",
    "    TopFunctionName = None\n",
    "    with open(file_path, 'r') as f:\n",
    "        ir_content = f.read()\n",
    "        ir_dict = slice_ir_by_function(ir_content)\n",
    "    FuncNodeDict = {}\n",
    "    for fn_name, fn_content in ir_dict.items():\n",
    "        NodeDict = {}\n",
    "        Lines = ParseIRFile(fn_content)\n",
    "        for Line in Lines:    \n",
    "            NodeList = LineToDict(Line)\n",
    "            if NodeList != None:\n",
    "                for NodeObj in NodeList:\n",
    "                    NodeDict[NodeObj.name] = NodeObj\n",
    "            if \"top\" in Line:\n",
    "                TopFunctionName = fn_name\n",
    "        FuncNodeDict[fn_name] = NodeDict\n",
    "    SubCounter = 0\n",
    "    NodeCounter = 0\n",
    "    JsonOutDict = {}\n",
    "    G_goble = nx.DiGraph()\n",
    "    for fn_name, fn_nodes in FuncNodeDict.items():\n",
    "        G = nx.DiGraph()\n",
    "        G_goble = DictToGraph(G_goble, fn_nodes)\n",
    "        \n",
    "    return G_goble\n",
    "\n",
    "\n",
    "def read_SDC_pipeline_result(file_path):\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        file_contents = file.read()\n",
    "\n",
    "    schedule_dict = {}\n",
    "    current_stage = None\n",
    "    current_node = None\n",
    "    \n",
    "    # Regular expressions to match the lines\n",
    "    function_pattern = re.compile(r'^function: \"(.*)\"')\n",
    "    stage_pattern = re.compile(r'^\\s*stage: (\\d+)')\n",
    "    node_pattern = re.compile(r'^\\s*node: \"(.*)\"')\n",
    "    node_delay_pattern = re.compile(r'^\\s*node_delay_ps: (\\d+)')\n",
    "    path_delay_pattern = re.compile(r'^\\s*path_delay_ps: (\\d+)')\n",
    "\n",
    "    for line in file_contents.splitlines():\n",
    "        # Check for function\n",
    "        function_match = function_pattern.match(line)\n",
    "        if function_match:\n",
    "            schedule_dict['function'] = function_match.group(1)\n",
    "            schedule_dict['stages'] = []\n",
    "            continue\n",
    "        \n",
    "        # Check for stage\n",
    "        stage_match = stage_pattern.match(line)\n",
    "        if stage_match:\n",
    "            current_stage = {'stage': int(stage_match.group(1)), 'timed_nodes': []}\n",
    "            schedule_dict['stages'].append(current_stage)\n",
    "            continue\n",
    "        \n",
    "        # Check for node\n",
    "        node_match = node_pattern.match(line)\n",
    "        if node_match:\n",
    "            current_node = {'node': node_match.group(1)}\n",
    "            current_stage['timed_nodes'].append(current_node)\n",
    "            continue\n",
    "        \n",
    "        # Check for node delay\n",
    "        node_delay_match = node_delay_pattern.match(line)\n",
    "        if node_delay_match:\n",
    "            current_node['node_delay_ps'] = int(node_delay_match.group(1))\n",
    "            continue\n",
    "        \n",
    "        # Check for path delay\n",
    "        path_delay_match = path_delay_pattern.match(line)\n",
    "        if path_delay_match:\n",
    "            current_node['path_delay_ps'] = int(path_delay_match.group(1))\n",
    "            continue\n",
    "    \n",
    "    return schedule_dict\n",
    "\n",
    "def register_SDC_result(G, schedule_dict):\n",
    "    for cur_dict in schedule_dict['stages']:\n",
    "        cur_stage = cur_dict['stage']\n",
    "        node_list = cur_dict['timed_nodes']\n",
    "        for cur_node in node_list:\n",
    "            # Check if the last part is an integer\n",
    "            node_name_parts = cur_node['node'].split('.')\n",
    "            if node_name_parts[-1].isdigit():\n",
    "                # If it is, use it as the ID\n",
    "                cur_node_id = int(node_name_parts[-1])\n",
    "            else:\n",
    "                # If not, use the entire node name as the ID\n",
    "                cur_node_id = cur_node['node']\n",
    "            \n",
    "            G.nodes[cur_node_id]['node_delay_ps'] = cur_node['node_delay_ps']\n",
    "            G.nodes[cur_node_id]['path_delay_ps'] = cur_node['path_delay_ps']\n",
    "            G.nodes[cur_node_id]['stage'] = cur_stage\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_unify_name(ir_input_path, ir_unify_out):\n",
    "    command = [\n",
    "        \"/home/miao/xls/bazel-bin/xls/tools/UnifyName\",\n",
    "        ir_input_path,\n",
    "        ir_unify_out\n",
    "    ]\n",
    "    command_str = \" \".join(command)\n",
    "    try:\n",
    "        result = subprocess.run(command_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        result.check_returncode()  # This will raise an exception if the return code was non-zero\n",
    "        print(\"Scheduling Run Done.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"An error occurred while running the command.\")\n",
    "        print(\"Error message:\", e.stderr)\n",
    "    return\n",
    "\n",
    "def run_rewriter(command_executable, ir_input_path, json_output_path, ir_output_path):\n",
    "    print(\"\\nRunning Standalone Rewriter\")\n",
    "    command = f\"{command_executable} {ir_input_path} {json_output_path} {ir_output_path}\"\n",
    "    try:\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        result.check_returncode()\n",
    "        print(\"Rewriter Run Done\")\n",
    "#         print(\"Output:\", result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"An error occurred while running the command.\")\n",
    "        print(\"Error message:\", e.stderr)\n",
    "    return\n",
    "\n",
    "def run_sdc_scheduler(ir_output_path, output_verilog_path, schedule_result_path, output_schedule_ir_path, \n",
    "                      delay_model, TopFunctionName, clock_period_ps, clock_margin_precent, \n",
    "                      pipeline_stages, period_relaxation_percent):\n",
    "    command = [\n",
    "            \"/home/miao/xls/bazel-bin/xls/tools/codegen_main\",\n",
    "            ir_output_path,\n",
    "            '--generator=pipeline',  \n",
    "            f'--delay_model={delay_model}',\n",
    "            '--module_name=xls_test',  # Assuming module_name is static\n",
    "            f'--top={TopFunctionName}',\n",
    "            f'--output_verilog_path={output_verilog_path}',  \n",
    "            f'--output_schedule_path={schedule_result_path}',\n",
    "            f'--output_schedule_ir_path={output_schedule_ir_path}'  \n",
    "        ]\n",
    "        \n",
    "    if clock_period_ps != None:\n",
    "        command.append(f'--clock_period_ps={clock_period_ps}')\n",
    "        if clock_margin_precent != None:\n",
    "            command.append(f'--clock_margin_percent={clock_margin_precent}')\n",
    "    elif pipeline_stages != None:\n",
    "        command.append(f'--pipeline_stages={pipeline_stages}')\n",
    "        if period_relaxation_percent != None:\n",
    "            command.append(f'--period_relaxation_percent={period_relaxation_percent}')\n",
    "\n",
    "    command_str = \" \".join(command)\n",
    "    try:\n",
    "        result = subprocess.run(command_str, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        result.check_returncode()  # This will raise an exception if the return code was non-zero\n",
    "        print(\"Scheduling Run Done.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"An error occurred while running the command.\")\n",
    "        print(\"Error message:\", e.stderr)\n",
    "    return\n",
    "\n",
    "def get_graph_with_scheduling(output_schedule_ir_path, schedule_result_path):\n",
    "    print(\"Reading Scheduling Result\")\n",
    "    G_schedule = ReadScheduleIR(output_schedule_ir_path)\n",
    "    schedule_dict = read_SDC_pipeline_result(schedule_result_path)\n",
    "    G_schedule = register_SDC_result(G_schedule, schedule_dict)\n",
    "    print(\"Result Graph Generation Done\")\n",
    "    return G_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the operations as named tuples\n",
    "add = namedtuple(\"add\", \"x y\")\n",
    "sub = namedtuple(\"sub\", \"x y\")\n",
    "mul = namedtuple(\"mul\", \"x y\")\n",
    "div = namedtuple(\"div\", \"x y\")\n",
    "shll = namedtuple(\"shll\", \"x y\")  # Bitshift left\n",
    "shrl = namedtuple(\"shrl\", \"x y\")  # Bitshift right\n",
    "not_ = namedtuple(\"not_\", \"x\")    # Bitwise NOT\n",
    "neg = namedtuple(\"neg\", \"x\")      # Negation\n",
    "\n",
    "# Evaluation function\n",
    "def eval_ops(car, cdr):\n",
    "    try:\n",
    "        if isinstance(car, (int, float)):\n",
    "            return car\n",
    "\n",
    "        if len(cdr) == 0:\n",
    "            return None\n",
    "\n",
    "        op = car\n",
    "        args = cdr\n",
    "\n",
    "        a = args[0]\n",
    "        if op == not_:\n",
    "            return ~a\n",
    "        if op == neg:\n",
    "            return -a\n",
    "\n",
    "        b = args[1]\n",
    "        if op == add:\n",
    "            return a + b\n",
    "        if op == sub:\n",
    "            return a - b\n",
    "        if op == mul:\n",
    "            return a * b\n",
    "        if op == div and b != 0:\n",
    "            return a / b\n",
    "        if op == shll:\n",
    "            return a << b\n",
    "        if op == shrl:\n",
    "            return a >> b\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return None\n",
    "\n",
    "# Rewrite rules\n",
    "a, b, c = vars(\"a b c\")  # Variables for rewrite rules\n",
    "\n",
    "list_rules: List[List[Any]] = [\n",
    "    # DAC paper rules\n",
    "    [\"mul_associativity\", mul(mul(a, b), c), mul(b, mul(a, c))],\n",
    "    [\"add_associativity\", add(add(a, b), c), add(b, add(a, c))],\n",
    "    [\"mul_distributivity\", mul(a, add(b, c)), add(mul(a, b), mul(a, c))],\n",
    "    [\"sum_same\", add(a, a), mul(a, 2)],\n",
    "    [\"mul_sum_same\", add(mul(a, b), b), mul(add(a, 1), b)],\n",
    "    [\"sub_to_neg\", sub(a, b), add(a, neg(b))],\n",
    "    [\"mul_by_two\", mul(a, 2), shll(a, 1)],\n",
    "    [\"merge_left_shift\", shll(shll(a, b), c), shll(a, add(b, c))],\n",
    "    [\"merge_right_shift\", shrl(shrl(a, b), c), shrl(a, add(b, c))],\n",
    "    [\"neg_to_not\", neg(a), add(not_(a), 1)],\n",
    "    \n",
    "    # Basic arithmetic rules\n",
    "    [\"comm_add\", add(a, b), add(b, a)],\n",
    "    [\"comm_mul\", mul(a, b), mul(b, a)],\n",
    "    [\"zero_add\", add(a, 0), a],\n",
    "    [\"zero_mul\", mul(a, 0), 0],\n",
    "    [\"one_mul\", mul(a, 1), a],\n",
    "    [\"sub_zero\", sub(a, 0), a],\n",
    "    [\"neg\", neg(a), sub(0, a)],\n",
    "\n",
    "    # Bitwise rules\n",
    "    [\"shll_zero\", shll(a, 0), a],\n",
    "    [\"shrl_zero\", shrl(a, 0), a],\n",
    "    [\"not_not\", not_(not_(a)), a],\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "# Convert list rules into rewrites\n",
    "rules = [Rewrite(frm, to, name) for name, frm, to in list_rules]\n",
    "\n",
    "# Function to check if two expressions are equivalent\n",
    "def is_equal(expr_a, expr_b, iters=5):\n",
    "    egraph = EGraph(eval_bitwise_ops)\n",
    "    id_a = egraph.add(expr_a)\n",
    "    id_b = egraph.add(expr_b)\n",
    "    egraph.run(rules, iters)\n",
    "    return egraph.equiv(id_a, id_b)\n",
    "\n",
    "def get_egraph_in_range(expr, min_iters=0, max_iters=50):\n",
    "    expr_list = []\n",
    "    # Create an EGraph with the evaluation function\n",
    "    egraph = EGraph(eval_ops)\n",
    "\n",
    "    # Add the expression to the e-graph\n",
    "    expr_id = egraph.add(expr)\n",
    "\n",
    "    # Run the e-graph with the rewrite rules\n",
    "    for cur_iters in range(min_iters, max_iters):\n",
    "        egraph.run(rules, cur_iters)\n",
    "        cur_expr = egraph.extract(expr_id)\n",
    "        if cur_expr in expr_list:\n",
    "            break\n",
    "        else:\n",
    "            expr_list.append(cur_expr)\n",
    "\n",
    "    return expr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entries(operand_list, to_remove):\n",
    "    return [op for op in operand_list if op not in to_remove]\n",
    "\n",
    "def traverse_graph(G, start_node, processed_nodes):\n",
    "# This function takes in a graph and partitions it to the largest legal rewrite batch\n",
    "# If a node is not in the allowed list, it is discarded.\n",
    "    allowed_operations = [\"add\", \"umul\", \"smul\", \"literal\", \"shll\", \"shrl\", \"not\", \"neg\", \"sub\", \"udiv\", \"sdiv\"]\n",
    "    # Initialize the batch list\n",
    "    batch = []\n",
    "\n",
    "    # Create a set to keep track of visited nodes\n",
    "    visited = set()\n",
    "\n",
    "    # Traverse the graph\n",
    "    stack = [start_node]\n",
    "    while stack:\n",
    "#         print(stack)\n",
    "        current = stack.pop()\n",
    "        current_op_type = G.nodes[current]['OperationType']\n",
    "\n",
    "        # Check if the current node's operation is allowed\n",
    "        if current_op_type in allowed_operations:\n",
    "            # Add the current node to the batch and mark as visited\n",
    "            batch.append(current)\n",
    "            visited.add(current)\n",
    "\n",
    "            for parent in G.predecessors(current):\n",
    "                # Check if all children of the parent are either visited or disallowed\n",
    "                all_children_allowed = all(\n",
    "                    child in visited # or G.nodes[child].get('OperationType', None) not in allowed_operations\n",
    "                    for child in G.successors(parent)\n",
    "                )\n",
    "\n",
    "                if all_children_allowed and parent not in processed_nodes and parent not in visited:\n",
    "                    stack.append(parent)\n",
    "    if len(batch) == 1 and G.nodes[batch[0]]['OperationType'] == 'literal':\n",
    "        batch = []\n",
    "    return batch\n",
    "\n",
    "def get_expr_from_batch(G, batch):\n",
    "    N = len(batch)\n",
    "    max_loop_count = N * (N + 1) // 2\n",
    "    batch_queue = deque(batch)\n",
    "    visited_dict = {}\n",
    "    loop_count = 0\n",
    "    cur_expr = None\n",
    "    is_unsigned = True\n",
    "    while batch_queue:\n",
    "        cur_expr = None\n",
    "        cur_node = batch_queue.pop()\n",
    "        # If all operands are not in the batch waiting the be handled, or they have been vistied before\n",
    "        if all(operand not in batch or operand in visited_dict for operand in G.nodes[cur_node]['Operands']):\n",
    "            if G.nodes[cur_node]['OperationType'] == 'literal':\n",
    "                visited_dict[G.nodes[cur_node]['OperationName']] = str(G.nodes[cur_node]['Value'])\n",
    "            else:\n",
    "                cur_expr = G.nodes[cur_node]['OperationType'] + \"(\"\n",
    "                for operand in G.nodes[cur_node]['Operands']:\n",
    "                    # If the operand belongs to a processed expr, use it. Otherwise use the operand as string\n",
    "                    if operand in visited_dict.keys():\n",
    "                        cur_expr = cur_expr + visited_dict[operand] + \",\"\n",
    "                    else:\n",
    "                        cur_expr = cur_expr + \"'\" + operand + \"'\" + \",\"\n",
    "                cur_expr = cur_expr[:-1] + \")\"\n",
    "                visited_dict[G.nodes[cur_node]['OperationName']] = cur_expr\n",
    "            \n",
    "            if G.nodes[cur_node]['OperationType'] in [\"neg\", \"smul\", \"sdiv\"]:\n",
    "                is_unsigned = False\n",
    "            \n",
    "        else:\n",
    "            batch_queue.appendleft(cur_node)\n",
    "            \n",
    "        if loop_count > max_loop_count:\n",
    "            break\n",
    "        else:\n",
    "            loop_count += 1\n",
    "    \n",
    "    # Handle sign mapping, handle not mapping\n",
    "    cur_expr = cur_expr.replace(\"umul\", \"mul\").replace(\"smul\", \"mul\")\n",
    "    cur_expr = cur_expr.replace(\"udiv\", \"div\").replace(\"sdiv\", \"div\")\n",
    "    cur_expr = cur_expr.replace(\"not\", \"not_\")\n",
    "    if len(batch_queue) == 0: \n",
    "        # Return the one with the most dependency\n",
    "        return cur_expr, is_unsigned\n",
    "    else:\n",
    "        print(\"[ERROR] Unresolved dependency in batch to expr generation\")\n",
    "        return None, None\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def gen_json_from_expr_recursive(G, expr, operation_mapping, bit_width, nodes_involved_dict=None, counter=None):\n",
    "    if counter is None:\n",
    "        counter = 0\n",
    "    if nodes_involved_dict is None:\n",
    "        nodes_involved_dict = {}\n",
    "    \n",
    "    match = re.search(r'\\b(\\w+)\\(([^()]+)\\)', expr)\n",
    "    if not match:\n",
    "        return nodes_involved_dict, expr\n",
    "\n",
    "    operation_type, operands = match.groups()\n",
    "    operation_name = 'auto_gen'+str(counter)\n",
    "\n",
    "    # Replace the innermost expression with the placeholder\n",
    "    new_expr = expr.replace(match.group(0), operation_name)\n",
    "#     print(\"OpType: \", operation_type, \"; Operands: \",operands)\n",
    "#     print(\"Iter:\", counter, \"expr: \",new_expr)\n",
    "    \n",
    "    \n",
    "    # Handle each node:\n",
    "    nodes_involved_dict, counter = handle_node_generation(G, operation_type, operands, operation_name, bit_width, nodes_involved_dict, operation_mapping, counter)\n",
    "    return gen_json_from_expr_recursive(G, new_expr, operation_mapping, bit_width, nodes_involved_dict, counter+1)\n",
    "\n",
    "\n",
    "# Node(name=None, bitwidth=None, operation=None, operands=None, \n",
    "#                  idNum=None, value=None, pos=None, FuncIO=None, start=None, width=None)\n",
    "\n",
    "def handle_node_generation(G, operation_type, operands, operation_name, bit_width, nodes_involved_dict, operation_mapping, counter):\n",
    "    operands_list = operands.split(',')\n",
    "    new_node_operands = []\n",
    "    NewNode = None\n",
    "    for cur_operand in operands_list:\n",
    "        cur_operand = cur_operand.split(\"=\")[-1]\n",
    "        # First setup operand list\n",
    "        if cur_operand.strip(\"'\").strip(\" \").isdigit():\n",
    "            counter += 1\n",
    "            # Handle pure digit, generate literal from them\n",
    "            try:\n",
    "                cur_value = int(cur_operand.strip(\"'\").strip(\" \"))\n",
    "            except ValueError:\n",
    "                cur_value = float(cur_operand.strip(\"'\").strip(\" \"))\n",
    "            NewLiteralNode = node_to_dict(Node('auto_gen'+str(counter), bit_width, 'Literal', value=cur_value))\n",
    "            NewLiteralNode[\"ReplaceSelfWith\"] = 'Gen'       \n",
    "            nodes_involved_dict['auto_gen'+str(counter)] = NewLiteralNode\n",
    "            new_node_operands.append('auto_gen'+str(counter))\n",
    "        else:\n",
    "            new_node_operands.append(cur_operand.strip(\"'\"))\n",
    "    #Now generate new node:\n",
    "    NewNode = node_to_dict(Node(operation_name, bit_width, operation_mapping[operation_type], new_node_operands))\n",
    "    NewNode[\"ReplaceSelfWith\"] = 'Gen'\n",
    "    nodes_involved_dict[operation_name] = NewNode\n",
    "    return nodes_involved_dict, counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json_from_ir(ir_input_path, json_output_path, rewrite_rule = \"NoRule\"):\n",
    "    # This function takes in ir file and generate json instruction based on e-graph\n",
    "    \n",
    "    TopFunctionName = None\n",
    "    with open(ir_input_path, 'r') as f:\n",
    "        ir_content = f.read()\n",
    "        ir_dict = slice_ir_by_function(ir_content)\n",
    "\n",
    "    #     print(ir_content)\n",
    "\n",
    "    FuncNodeDict = {}\n",
    "    for fn_name, fn_content in ir_dict.items():\n",
    "        NodeDict = {}\n",
    "        Lines = ParseIRFile(fn_content)\n",
    "        for Line in Lines:    \n",
    "            NodeList = LineToDict(Line)\n",
    "            if NodeList != None:\n",
    "                for NodeObj in NodeList:\n",
    "                    NodeDict[NodeObj.name] = NodeObj\n",
    "            if \"top\" in Line:\n",
    "                TopFunctionName = fn_name\n",
    "        FuncNodeDict[fn_name] = NodeDict\n",
    "    SubCounter = 0\n",
    "    NodeCounter = 0\n",
    "    JsonOutDict = {}\n",
    "#     print(FuncNodeDict)\n",
    "\n",
    "\n",
    "    ## The following code reads digraph and partitions them into legal rewrite batches\n",
    "    graph_by_function = {}\n",
    "    sorted_nodes_by_function = {}\n",
    "\n",
    "    for fn_name, fn_nodes in FuncNodeDict.items():\n",
    "        G = nx.DiGraph()\n",
    "        G = DictToGraph(G, fn_nodes)\n",
    "        graph_by_function[fn_name] = G\n",
    "        \n",
    "#         nx.draw(G, with_labels=True, font_weight='bold')\n",
    "\n",
    "        # Check if the graph is a DAG\n",
    "        if nx.is_directed_acyclic_graph(G):\n",
    "            sorted_nodes = list(nx.topological_sort(G))\n",
    "            sorted_nodes_by_function[fn_name] = sorted_nodes[::-1]\n",
    "        else:\n",
    "            sorted_nodes_by_function[fn_name] = None\n",
    "            print(f\"[Warning] Graph for function {fn_name} is not a DAG. Topological sorting cannot be performed.\")\n",
    "\n",
    "#         print(f\"Function: {fn_name}\")\n",
    "#         print(\"Batches of nodes:\", sorted_nodes_by_function[fn_name])   \n",
    "\n",
    "\n",
    "    rewrite_batch_by_function = {}\n",
    "    processed_nodes = set()\n",
    "\n",
    "    for fn_name, nodes in sorted_nodes_by_function.items():\n",
    "        G = graph_by_function[fn_name]\n",
    "        rewrite_batch_by_function[fn_name] = []\n",
    "\n",
    "        temp_nodes = nodes.copy()\n",
    "        cur_index = 0\n",
    "        while temp_nodes and cur_index < len(temp_nodes):\n",
    "            start_node = temp_nodes[cur_index]\n",
    "            cur_batch = traverse_graph(G, start_node, processed_nodes)\n",
    "#             print(\"Current  Batch: \", cur_batch)\n",
    "            if cur_batch != []:\n",
    "                rewrite_batch_by_function[fn_name].append(cur_batch)\n",
    "                for node in cur_batch:\n",
    "                    processed_nodes.add(node)\n",
    "                    if node in temp_nodes:\n",
    "                        temp_nodes.remove(node)\n",
    "            else:\n",
    "                cur_index += 1\n",
    "\n",
    "        print(f\"Function: {fn_name}\")\n",
    "        print(\"Batches of nodes:\", rewrite_batch_by_function[fn_name])\n",
    "\n",
    "    ## The following code takes each batch, and generate expression for e-graph search\n",
    "    rewrite_candidate_by_function = {}\n",
    "    for fn_name, batch_list in rewrite_batch_by_function.items():\n",
    "        G = graph_by_function[fn_name]\n",
    "        rewrite_candidate_by_function[fn_name] = {}\n",
    "        rewrite_candidate_by_function[fn_name]['is_unsigned'] = []\n",
    "        rewrite_candidate_by_function[fn_name]['rewrite_candidates'] = []\n",
    "        for batch in batch_list:\n",
    "            batch_expr, is_unsigned = get_expr_from_batch(G, batch)\n",
    "#             print(\"FunctionName: \", fn_name,\"Input Expr: \", batch_expr)\n",
    "            cur_candidates = get_egraph_in_range(eval(batch_expr))\n",
    "            rewrite_candidate_by_function[fn_name]['is_unsigned'].append(is_unsigned)\n",
    "            rewrite_candidate_by_function[fn_name]['rewrite_candidates'] .append(cur_candidates)\n",
    "\n",
    "    # print(rewrite_candidate_by_function)\n",
    "\n",
    "    operation_mapping_unsigned = {\"add\" : \"kAdd\",\n",
    "                                  \"mul\" : \"kUMul\",\n",
    "                                  \"div\" : \"kUDiv\",\n",
    "                                  \"sub\" : \"kSub\",\n",
    "                                \"literal\" : \"Literal\",\n",
    "                                 \"neg\" : \"kNeg\",\n",
    "                                 \"shll\" : \"kShll\",\n",
    "                                 \"shrl\" : \"kShrl\",\n",
    "                                 \"not_\" : \"kNot\"}\n",
    "\n",
    "    operation_mapping_signed = {\"add\" : \"kAdd\",\n",
    "                                  \"mul\" : \"kSMul\",\n",
    "                                  \"div\" : \"kSDiv\",\n",
    "                                  \"sub\" : \"kSub\",\n",
    "                                \"literal\" : \"Literal\",\n",
    "                                 \"neg\" : \"kNeg\",\n",
    "                                 \"shll\" : \"kShll\",\n",
    "                                 \"shrl\" : \"kShrl\",\n",
    "                                 \"not_\" : \"kNot\"}\n",
    "    json_dict = {}\n",
    "    counter = 0\n",
    "    for fn_name, all_rewrite_candidates_dict in rewrite_candidate_by_function.items():\n",
    "        G = graph_by_function[fn_name]\n",
    "        for i in range(len(all_rewrite_candidates_dict['rewrite_candidates'])):\n",
    "            cur_rewrite_candidate = all_rewrite_candidates_dict['rewrite_candidates'][i]\n",
    "            \n",
    "            #First we initialize the dictionary\n",
    "            json_dict[str(counter)] = {}\n",
    "            json_dict[str(counter)][\"FuncName\"] = fn_name\n",
    "\n",
    "            #Then, find bitwidth and if the variables are signed, and assign operator mapping\n",
    "            cur_is_unsigned = all_rewrite_candidates_dict['is_unsigned'][i]\n",
    "            bit_width = G.nodes[rewrite_batch_by_function[fn_name][i][0]]['BitWidth']\n",
    "            if cur_is_unsigned:\n",
    "                cur_operation_mapping = operation_mapping_unsigned\n",
    "            else:\n",
    "                cur_operation_mapping = operation_mapping_signed\n",
    "                \n",
    "                \n",
    "            # Now we call our selector to get a best rewrite\n",
    "            if rewrite_rule == \"NoRule\":\n",
    "                rewrite_expr = cur_rewrite_candidate[-1]\n",
    "            elif rewrite_rule == \"NativePick\":\n",
    "                rewrite_expr = native_pick(G, cur_rewrite_candidate, bit_width, cur_operation_mapping)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "#             print(\"FunctionName: \",fn_name, \"\\nRewrite Expr\", rewrite_expr)\n",
    "\n",
    "            #Next, handle node generation\n",
    "            nodes_involved_dict, out_node_name = gen_json_from_expr_recursive(G, str(rewrite_expr), cur_operation_mapping, bit_width)\n",
    "\n",
    "            #Next, handle node elimination\n",
    "            for old_node_id in rewrite_batch_by_function[fn_name][i]:\n",
    "                nodes_involved_dict[G.nodes[old_node_id]['OperationName']] = G.nodes[old_node_id]\n",
    "                nodes_involved_dict[G.nodes[old_node_id]['OperationName']]['ReplaceSelfWith'] = 'Kill'\n",
    "\n",
    "            #Next, handle batch output replacement\n",
    "            #ToDo: we need to verify the first is always the output of the batch\n",
    "            nodes_involved_dict[G.nodes[rewrite_batch_by_function[fn_name][i][0]]['OperationName']] = G.nodes[rewrite_batch_by_function[fn_name][i][0]]\n",
    "            nodes_involved_dict[G.nodes[rewrite_batch_by_function[fn_name][i][0]]['OperationName']]['ReplaceSelfWith'] = str(out_node_name)\n",
    "\n",
    "            json_dict[str(counter)][\"NodesInvolved\"] = list(nodes_involved_dict.values())\n",
    "            counter += 1\n",
    "            \n",
    "#             print(nodes_involved_dict)\n",
    "\n",
    "    with open(json_output_path, 'w') as json_file:\n",
    "            json.dump(json_dict, json_file, indent=4)  \n",
    "    return G, TopFunctionName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egraph_flow(ir_input_path, delay_model='sky130', \n",
    "               Schedule_Method=\"SDC\", clock_period_ps=None, clock_margin_precent=None, \n",
    "               pipeline_stages=None, period_relaxation_percent=None):\n",
    "    \n",
    "    command_executable = '/home/miao/xls/bazel-bin/xls/tools/RL_main'\n",
    "    ir_unify_name_out = ir_input_path.replace('.ir','.unify.ir')\n",
    "    json_output_path = ir_input_path.replace('.ir','.json')\n",
    "    ir_output_path = ir_input_path.replace('.ir','_substitution.ir')\n",
    "    schedule_result_path = ir_input_path.replace('.ir', '_schedule.txt')\n",
    "    output_verilog_path = json_output_path.replace(\".json\", \".v\")\n",
    "    output_schedule_ir_path = ir_output_path.replace(\".ir\", \"_schedule.ir\")\n",
    "    \n",
    "    run_unify_name(ir_input_path, ir_unify_name_out)\n",
    "    #Get rewrite json file from IR and selector model\n",
    "    G, TopFunctionName = gen_json_from_ir(ir_unify_name_out, json_output_path)\n",
    "    #Run cc rewriter to implement the rewrite\n",
    "    run_rewriter(command_executable, ir_unify_name_out, json_output_path, ir_output_path)\n",
    "    #Run SDC with the new IR and generate result.\n",
    "    run_sdc_scheduler(ir_output_path, output_verilog_path, schedule_result_path, output_schedule_ir_path, \n",
    "                      delay_model, TopFunctionName, clock_period_ps, clock_margin_precent, \n",
    "                      pipeline_stages, period_relaxation_percent)\n",
    "    #Collect the scheduling result to a graph\n",
    "    G_schedule = get_graph_with_scheduling(output_schedule_ir_path, schedule_result_path)\n",
    "    \n",
    "    return G_schedule\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gen_json_from_ir(\"/home/miao/xls/work_space/EGraphUnitTest/all_unit_test.ir\", \"/home/miao/xls/work_space/EGraphUnitTest/all_unit_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduling Run Done.\n",
      "Function: sha256main\n",
      "Batches of nodes: [[37936, 51596, 37949, 37948, 37955, 37947], [51191, 51367], [37953, 37951], [51580], [51181, 51365], [51196, 51368], [51186, 51366], [51578], [51176, 51364]]\n",
      "\n",
      "Running Standalone Rewriter\n",
      "Rewriter Run Done\n",
      "Scheduling Run Done.\n",
      "Reading Scheduling Result\n",
      "Result Graph Generation Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x7fb4906f7fa0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# egraph_flow(\"/home/miao/xls/work_space/EGraphTest/test.opt.ir\", clock_period_ps = 1000)\n",
    "# egraph_flow(\"/home/miao/xls/work_space/EGraphUnitTest/all_unit_test.ir\", clock_period_ps = 1000)\n",
    "egraph_flow(\"/home/miao/xls/work_space/Sha256/sha256.opt.ir\", clock_period_ps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The subsequent parts are for generating graphs for each rewrite expressions,\n",
    "# as well as testing ways to use these graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def native_pick(G, cur_rewrite_candidate, bit_width, cur_operation_mapping):\n",
    "    graph_list = []\n",
    "    for cur_expr in cur_rewrite_candidate:\n",
    "#         print(cur_expr)\n",
    "        cur_graph = expr_to_graph(G, cur_expr,  bit_width, cur_operation_mapping)\n",
    "        graph_list.append(cur_graph)\n",
    "#         print(cur_graph.nodes)\n",
    "    \n",
    "    best_rewrite = cur_rewrite_candidate[-1]\n",
    "    return best_rewrite\n",
    "\n",
    "def expr_to_graph(G, expr, bit_width, cur_operation_mapping):\n",
    "    cur_graph = nx.DiGraph()\n",
    "    nodes_involved_dict, out_node_name = gen_json_from_expr_recursive(G, str(expr), cur_operation_mapping, bit_width)\n",
    "    filtered_dict = {k: v for k, v in nodes_involved_dict.items() if v['ReplaceSelfWith'] == 'Gen'}\n",
    "    node_obj_dict = dict_to_nodes(filtered_dict)\n",
    "    node_name_list = list(node_obj_dict.keys())\n",
    "    new_nodes = {}\n",
    "    # Iterate over the nodes and check for missing operands\n",
    "    for node_name, node_obj in node_obj_dict.items():\n",
    "        if node_obj.operands != None:\n",
    "            for operand in node_obj.operands:\n",
    "                if operand not in node_name_list and operand not in new_nodes:\n",
    "                    # Create a new Node object for the missing operand\n",
    "                    new_nodes[operand] = Node(name=operand, operands=[])\n",
    "        else:\n",
    "            node_obj.operands = []\n",
    "        \n",
    "    # Merge the new nodes into the main dictionary\n",
    "    node_obj_dict.update(new_nodes)\n",
    "    counter = 0\n",
    "    for node_name, node_obj in node_obj_dict.items():\n",
    "        node_obj.idNum = counter\n",
    "        counter += 1\n",
    "    cur_graph = DictToGraph(cur_graph, node_obj_dict)\n",
    "    return cur_graph\n",
    "\n",
    "def dict_to_nodes(node_dict):\n",
    "    node_objects = {}\n",
    "    for node_name, node_data in node_dict.items():\n",
    "        # Create Node object using the data from the dictionary\n",
    "        node = Node(\n",
    "            name=node_data.get(\"OperationName\"),\n",
    "            bitwidth=node_data.get(\"BitWidth\"),\n",
    "            operation=node_data.get(\"OperationType\"),\n",
    "            operands=node_data.get(\"Operands\"),\n",
    "            idNum=node_data.get(\"idNum\"),\n",
    "            value=node_data.get(\"Value\"),\n",
    "            pos=node_data.get(\"Pos\"),\n",
    "            FuncIO=node_data.get(\"FuncIO\"),\n",
    "            start=node_data.get(\"Start\"),\n",
    "            width=node_data.get(\"Width\"),\n",
    "            array_sizes=node_data.get(\"ArraySize\"),\n",
    "            indices=node_data.get(\"Indices\")\n",
    "        )\n",
    "        # Store the Node object using the node name as the key\n",
    "        node_objects[node_name] = node\n",
    "    return node_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: sha256main\n",
      "Batches of nodes: [[37936, 51596, 37949, 37948, 37955, 37947], [51191, 51367], [37953, 37951], [51580], [51181, 51365], [51196, 51368], [51186, 51366], [51578], [51176, 51364]]\n"
     ]
    }
   ],
   "source": [
    "G, FuncName = gen_json_from_ir(\"/home/miao/xls/work_space/Sha256/sha256.opt.ir\", \"/home/miao/xls/work_space/Sha256/sha256.json\", rewrite_rule=\"NativePick\")\n",
    "# nx.draw(G, with_labels=True, font_weight='bold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The subsequent part is for getting SDC results and testing GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduling Run Done.\n",
      "Function: sha256main\n",
      "Batches of nodes: [[37936, 51596, 37949, 37948, 37955, 37947], [51191, 51367], [37953, 37951], [51580], [51181, 51365], [51196, 51368], [51186, 51366], [51578], [51176, 51364]]\n",
      "Scheduling Run Done.\n",
      "Reading Scheduling Result\n",
      "Result Graph Generation Done\n"
     ]
    }
   ],
   "source": [
    "def get_sdc_from_origin_graph(ir_input_path, delay_model='sky130', \n",
    "               Schedule_Method=\"SDC\", clock_period_ps=None, clock_margin_precent=None, \n",
    "               pipeline_stages=None, period_relaxation_percent=None):\n",
    "    \n",
    "    command_executable = '/home/miao/xls/bazel-bin/xls/tools/RL_main'\n",
    "    ir_unify_name_out = ir_input_path.replace('.ir','.unify.ir')\n",
    "    json_output_path = ir_input_path.replace('.ir','.json')\n",
    "    ir_output_path = ir_input_path.replace('.ir','_substitution.ir')\n",
    "    schedule_result_path = ir_input_path.replace('.ir', '_schedule.txt')\n",
    "    output_verilog_path = json_output_path.replace(\".json\", \".v\")\n",
    "    output_schedule_ir_path = ir_output_path.replace(\".ir\", \"_schedule.ir\")\n",
    "    \n",
    "    run_unify_name(ir_input_path, ir_unify_name_out)\n",
    "    #Get rewrite json file from IR and selector model\n",
    "    G, TopFunctionName = gen_json_from_ir(ir_unify_name_out, json_output_path)\n",
    "    #Run cc rewriter to implement the rewrite\n",
    "#     run_rewriter(command_executable, ir_unify_name_out, json_output_path, ir_output_path)\n",
    "    #Run SDC with the new IR and generate result.\n",
    "    run_sdc_scheduler(ir_unify_name_out, output_verilog_path, schedule_result_path, output_schedule_ir_path, \n",
    "                      delay_model, TopFunctionName, clock_period_ps, clock_margin_precent, \n",
    "                      pipeline_stages, period_relaxation_percent)\n",
    "    #Collect the scheduling result to a graph\n",
    "    G_schedule = get_graph_with_scheduling(output_schedule_ir_path, schedule_result_path)\n",
    "    \n",
    "    return G_schedule\n",
    "\n",
    "G = get_sdc_from_origin_graph(\"/home/miao/xls/work_space/Sha256/sha256.opt.ir\", clock_period_ps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView(('message', 51545, 51368, 51196))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2422e-01, -9.5573e-02, -7.2818e-02,  2.6223e-02, -1.7320e-02,\n",
      "          1.0703e-01, -1.2200e-01, -7.3209e-03,  1.0171e-02, -1.1234e-01],\n",
      "        [ 1.7605e-01, -8.4491e-02, -9.0774e-02,  1.7452e-02, -3.4899e-02,\n",
      "          8.3595e-02, -1.4160e-01,  4.1034e-02,  3.1321e-02, -6.1246e-02],\n",
      "        [ 2.7142e-01, -8.9219e-02, -1.6999e-01,  6.3407e-02, -1.5608e-01,\n",
      "         -2.0836e-02, -3.9519e-02,  1.1159e-01,  1.6087e-02, -1.3464e-02],\n",
      "        [ 3.4153e+01, -1.9950e-01, -7.4751e+00,  5.2218e+00, -8.0853e+00,\n",
      "         -2.1942e+01,  5.6587e-01,  1.0262e+01,  9.5569e+00,  3.0091e+01]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# List of allowed operations\n",
    "mapping_operations = [\"add\", \"umul\", \"smul\", \"literal\", \"shll\", \"shrl\", \"not\", \n",
    "                      \"neg\", \"sub\", \"udiv\", \"sdiv\", \"bit_slice\", \"concat\", None]\n",
    "\n",
    "# Create a mapping for one-hot encoding\n",
    "op_mapping = {op: i for i, op in enumerate(mapping_operations)}\n",
    "num_ops = len(mapping_operations)\n",
    "\n",
    "# Create a mapping of nodes to integers\n",
    "node_mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "num_nodes = len(node_mapping)\n",
    "\n",
    "# Convert edges using the mapping\n",
    "edges = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in G.edges()], dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Convert node attributes and include 'node_delay_ps', 'path_delay_ps', and one-hot encoded 'OperationType'\n",
    "x = []\n",
    "for node in G.nodes():\n",
    "    node_features = [\n",
    "        G.nodes[node]['node_delay_ps'],\n",
    "        G.nodes[node]['path_delay_ps']\n",
    "    ]\n",
    "    op_type = G.nodes[node].get('OperationType')\n",
    "    op_vector = [0] * num_ops  # Initialize with zeros\n",
    "    if op_type is not None:\n",
    "        op_vector[op_mapping[op_type]] = 1  # Set the corresponding index to 1 for one-hot encoding\n",
    "    node_features.extend(op_vector)\n",
    "    x.append(node_features)\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "# Create PyTorch Geometric data\n",
    "data = Data(x=x, edge_index=edges)\n",
    "\n",
    "# Define the GNN Model\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(GNN, self).__init__()\n",
    "        # Adjust the input size to accommodate the operation type features\n",
    "        self.conv1 = GCNConv(data.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 32)\n",
    "        self.linear = torch.nn.Linear(32, n)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model (define 'n' as needed)\n",
    "n = 10  # Example value for 'n'\n",
    "model = GNN(n)\n",
    "\n",
    "# Example forward pass\n",
    "out = model(data)\n",
    "print(out)  # Should be (num_nodes, n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
